{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist=input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "training_epochs=15\n",
    "batch_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() #그래프에 있는 모든 텐서를 초기화\n",
    "x=tf.placeholder(tf.float32, [None,784])\n",
    "y=tf.placeholder(tf.float32, [None,10])\n",
    "w1=tf.get_variable(\"w1\", shape=[784,256],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1=tf.Variable(tf.random_normal([256]))\n",
    "L1=tf.nn.relu(tf.matmul(x,w1)+b1)\n",
    "\n",
    "w2=tf.get_variable(\"w2\", shape=[256,256],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2=tf.Variable(tf.random_normal([256]))\n",
    "L2=tf.nn.relu(tf.matmul(L1,w2)+b2)\n",
    "\n",
    "w3=tf.get_variable(\"w3\", shape=[256,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3=tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "hf= tf.matmul(L2,w3)+b3\n",
    "\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits\n",
    "               (logits=hf, labels=y))\n",
    "train=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 0.302518476\n",
      "cost: 0.112670959\n",
      "cost: 0.073724152\n",
      "cost: 0.050726363\n",
      "cost: 0.038476309\n",
      "cost: 0.029557962\n",
      "cost: 0.023343669\n",
      "cost: 0.019974819\n",
      "cost: 0.015692812\n",
      "cost: 0.013776262\n",
      "cost: 0.013276411\n",
      "cost: 0.012094405\n",
      "cost: 0.009797232\n",
      "cost: 0.007730206\n",
      "cost: 0.006915720\n",
      "acc: 0.9799\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost=0\n",
    "    total_batch=int(mnist.train.num_examples/batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batchxs,batchys=mnist.train.next_batch(batch_size)\n",
    "        myfeed={x:batchxs, y:batchys}\n",
    "        cv,_=sess.run([cost, train], feed_dict=myfeed)\n",
    "        avg_cost+=cv/total_batch\n",
    "    print('cost:','{:.9f}'.format(avg_cost))\n",
    "\n",
    "c_pre=tf.equal(tf.argmax(hf, 1), tf.argmax(y,1))\n",
    "acc=tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
    "print('acc:', sess.run(acc, feed_dict=\n",
    "                       {x:mnist.test.images, \n",
    "                        y:mnist.test.labels}))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() #그래프에 있는 모든 텐서를 초기화\n",
    "x=tf.placeholder(tf.float32, [None,784])\n",
    "y=tf.placeholder(tf.float32, [None,10])\n",
    "\n",
    "w1=tf.get_variable(\"w1\", shape=[784,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1=tf.Variable(tf.random_normal([512]))\n",
    "L1=tf.nn.relu(tf.matmul(x,w1)+b1)\n",
    "\n",
    "w2=tf.get_variable(\"w2\", shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2=tf.Variable(tf.random_normal([512]))\n",
    "L2=tf.nn.relu(tf.matmul(L1,w2)+b2)\n",
    "\n",
    "w3=tf.get_variable(\"w3\", shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3=tf.Variable(tf.random_normal([512]))\n",
    "L3=tf.nn.relu(tf.matmul(L2,w3)+b3)\n",
    "\n",
    "w4=tf.get_variable(\"w4\", shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4=tf.Variable(tf.random_normal([512]))\n",
    "L4=tf.nn.relu(tf.matmul(L3,w4)+b4)\n",
    "\n",
    "w5=tf.get_variable(\"w5\", shape=[512,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5=tf.Variable(tf.random_normal([10]))\n",
    "hf=tf.matmul(L4,w5)+b5\n",
    "\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits\n",
    "               (logits=hf, labels=y))\n",
    "train=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 0.311996528\n",
      "cost: 0.104747453\n",
      "cost: 0.070911684\n",
      "cost: 0.053524543\n",
      "cost: 0.041237607\n",
      "cost: 0.032379584\n",
      "cost: 0.031140135\n",
      "cost: 0.026516020\n",
      "cost: 0.022664479\n",
      "cost: 0.021438611\n",
      "cost: 0.019501703\n",
      "cost: 0.015484938\n",
      "cost: 0.018087821\n",
      "cost: 0.014699100\n",
      "cost: 0.011459405\n",
      "acc: 0.9815\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost=0\n",
    "    total_batch=int(mnist.train.num_examples/batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batchxs,batchys=mnist.train.next_batch(batch_size)\n",
    "        myfeed={x:batchxs, y:batchys}\n",
    "        cv,_=sess.run([cost, train], feed_dict=myfeed)\n",
    "        avg_cost+=cv/total_batch\n",
    "    print('cost:','{:.9f}'.format(avg_cost))\n",
    "\n",
    "c_pre=tf.equal(tf.argmax(hf, 1), tf.argmax(y,1))\n",
    "acc=tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
    "print('acc:', sess.run(acc, feed_dict=\n",
    "                       {x:mnist.test.images, \n",
    "                        y:mnist.test.labels}))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() #그래프에 있는 모든 텐서를 초기화\n",
    "x=tf.placeholder(tf.float32, [None,784])\n",
    "y=tf.placeholder(tf.float32, [None,10])\n",
    "\n",
    "keep_prob=tf.placeholder(tf.float32)\n",
    "\n",
    "w1=tf.get_variable(\"w1\", shape=[784,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1=tf.Variable(tf.random_normal([512]))\n",
    "L1=tf.nn.relu(tf.matmul(x,w1)+b1)\n",
    "L1=tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "w2=tf.get_variable(\"w2\", shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2=tf.Variable(tf.random_normal([512]))\n",
    "L2=tf.nn.relu(tf.matmul(L1,w2)+b2)\n",
    "L2=tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "\n",
    "w3=tf.get_variable(\"w3\", shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3=tf.Variable(tf.random_normal([512]))\n",
    "L3=tf.nn.relu(tf.matmul(L2,w3)+b3)\n",
    "L3=tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "\n",
    "w4=tf.get_variable(\"w4\", shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4=tf.Variable(tf.random_normal([512]))\n",
    "L4=tf.nn.relu(tf.matmul(L3,w4)+b4)\n",
    "L4=tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "\n",
    "w5=tf.get_variable(\"w5\", shape=[512,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5=tf.Variable(tf.random_normal([10]))\n",
    "hf=tf.matmul(L4,w5)+b5\n",
    "\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits\n",
    "               (logits=hf, labels=y))\n",
    "train=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 0.469294091\n",
      "cost: 0.174984427\n",
      "cost: 0.131413354\n",
      "cost: 0.108758807\n",
      "cost: 0.089315589\n",
      "cost: 0.081801631\n",
      "cost: 0.076275577\n",
      "cost: 0.070978409\n",
      "cost: 0.061649515\n",
      "cost: 0.060526415\n",
      "cost: 0.057595954\n",
      "cost: 0.051941357\n",
      "cost: 0.055163962\n",
      "cost: 0.048438833\n",
      "cost: 0.046458614\n",
      "acc: 0.9821\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost=0\n",
    "    total_batch=int(mnist.train.num_examples/batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batchxs,batchys=mnist.train.next_batch(batch_size)\n",
    "        myfeed={x:batchxs, y:batchys, keep_prob:0.7}\n",
    "        cv,_=sess.run([cost, train], feed_dict=myfeed)\n",
    "        avg_cost+=cv/total_batch\n",
    "    print('cost:','{:.9f}'.format(avg_cost))\n",
    "\n",
    "c_pre=tf.equal(tf.argmax(hf, 1), tf.argmax(y,1))\n",
    "acc=tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
    "print('acc:', sess.run(acc, feed_dict=\n",
    "                       {x:mnist.test.images, \n",
    "                        y:mnist.test.labels,\n",
    "                       keep_prob:1}))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mnist model based CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    " # CNN 구현\n",
    "    # shape 주는 부분이 까다롭다\n",
    "learning_rate=0.001\n",
    "training_epochs=15\n",
    "batch_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() #그래프에 있는 모든 텐서를 초기화\n",
    "\n",
    "x=tf.placeholder(tf.float32, [None, 28*28])\n",
    "ximg=tf.reshape(x, [-1, 28, 28, 1])#img 28*28*1(black/white)\n",
    "y=tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "w1=tf.Variable(tf.random_normal([3,3,1,32]))\n",
    "L1=tf.nn.conv2d(ximg, w1, strides=[1,1,1,1],padding='SAME')\n",
    "L1=tf.nn.relu(L1)\n",
    "L1=tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "# L1 이미지 shape (?,28,28,1)\n",
    "# conv 통해 ( ?,28,28,32 )\n",
    "# pooling (?,14,14,32) \n",
    "\n",
    "# 출력결과가 본래 이미지 ( 28*28 ) => 3,3 가 된다\n",
    "# 초기화 \n",
    "w2=tf.Variable(tf.random_normal([3,3,32,64]))\n",
    "L2=tf.nn.conv2d(L1, w2, strides=[1,1,1,1],padding='SAME')\n",
    "L2=tf.nn.relu(L2)\n",
    "L2=tf.nn.max_pool(L2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "#conv2d (?,14,14,64)\n",
    "#relu (?,14,14,64)\n",
    "#max_pool (?,7,7,64)\n",
    "L2_flat=tf.reshape(L2, [-1, 7*7*64])\n",
    "#reshape => (?, 7*7*64)\n",
    "\n",
    "\n",
    "w3=tf.get_variable(\"w3\", shape=[7*7*64, 10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b=tf.Variable(tf.random_normal([10]))\n",
    "logits=tf.matmul(L2_flat, w3)+b\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "train=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# 해석\n",
    "# 28 * 28 => 3 * 3 컨볼루션 32개 => [ 2*2 풀링 14*14 32개 ]\n",
    "# [ 2*2 풀링 14*14 32개 ] => 두번째 계층의 입력으로 들어간다 ( L2 (3*3))\n",
    "# ( L2 (3*3)) filter 64개 => [2*2 풀링 7*7 64개]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 1.128327116\n",
      "cost: 0.262962590\n",
      "cost: 0.184128785\n",
      "cost: 0.115805395\n",
      "cost: 0.080420983\n",
      "cost: 0.065815993\n",
      "cost: 0.046901781\n",
      "cost: 0.035622754\n",
      "cost: 0.036541005\n",
      "cost: 0.032254669\n",
      "cost: 0.028590647\n",
      "cost: 0.022439918\n",
      "cost: 0.016319586\n",
      "cost: 0.015530408\n",
      "cost: 0.022674823\n",
      "acc: 0.9838\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost=0\n",
    "    total_batch=int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batchxs,batchys=mnist.train.next_batch(batch_size)\n",
    "        myfeed={x:batchxs, y:batchys}\n",
    "        cv,_=sess.run([cost, train], feed_dict=myfeed)\n",
    "        avg_cost+=cv/total_batch\n",
    "    print('cost:','{:.9f}'.format(avg_cost))\n",
    "\n",
    "c_pre=tf.equal(tf.argmax(logits, 1), tf.argmax(y,1))\n",
    "acc=tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
    "print('acc:', sess.run(acc, feed_dict=\n",
    "                       {x:mnist.test.images, \n",
    "                        y:mnist.test.labels,\n",
    "                       }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
