{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# day 44\n",
    "### seq2seq : 기계 번역\n",
    "### 영단어 -> 한단어 로 번역\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S : 디코딩 입력의 시작\n",
    "# E : 디코딩 출력의 끝\n",
    "# P : 패딩, time step크기 보다 작은단어, 빈 자리를 채우는 문자\n",
    "# time step = 4 인경우, love => l,o,v,e\n",
    "# my => m,y,P,P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_arr = [c for c in \"SEPabcdefghijklmnopqrstuvwxyz단어나무놀이소녀목록사랑\"]\n",
    "char_arr # corpus\n",
    "# 이문자들 내에서 학습이 되어지고 예측이 되어진다 \n",
    "# 이 글자들을 벗어나는 단어는 있을수 없다, 사전집합내에서 예측한다\n",
    "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "num_dic # 문자 하나하나 마다 인덱스가 붙어나온다\n",
    "dic_len = len(num_dic) # total_length 41\n",
    "dic_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영단어를 한글로 번역하기 위한 학습용 데이터\n",
    "seq_data = [['word','word'],['tree','나무'],\n",
    "            ['game','놀이'],['girl','소녀'],\n",
    "            ['list','목록'],['love','사랑']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph\n",
    "\n",
    "def make_batch(seq_data): # (word,'PPPP') # seq_data 받아서 입력단어의 글자들을 하나씨 떼서 \n",
    "    input_batch=[]\n",
    "    output_batch=[]\n",
    "    target_batch=[]\n",
    "    for seq in seq_data:\n",
    "        # 인코더 셀의 입력값\n",
    "        input = [num_dic[n] for n in seq[0]] # word (번역해야할 단어가 들어가있다)\n",
    "        # 디코더 셀의 입력값\n",
    "        output = [num_dic[n] for n in ('S'+seq[1])] # pppp 앞에 S심볼을 붙여준다\n",
    "        target = [num_dic[n] for n in (seq[1]+'E')] # pppp 뒤에 E심볼을 붙여준다\n",
    "#         print(input)\n",
    "#         print(output)\n",
    "        input_batch.append(np.eye(dic_len)[input]) # matric 만들어준다\n",
    "        output_batch.append(np.eye(dic_len)[output])\n",
    "        target_batch.append(target) # target은 ohe 아니다 \n",
    "#         print(input_batch)\n",
    "# word로 시작한거 에 대해서 \n",
    "        print(target_batch)\n",
    "    return input_batch, output_batch, target_batch\n",
    "\n",
    "# 옵션설정 PART\n",
    "learning_rate = 0.01\n",
    "n_hidden = 128 # 셀에서 나가는게 128\n",
    "total_epoch = 100\n",
    "# 입력과 출력의 형태는 ohe으로 data크기가 같다 # 41차원\n",
    "# 128, 41 [fully Conneceted layer. shape]\n",
    "n_class=n_input=dic_len # 입출력에 쓸 데이터 데이터형식 # 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-41-2226f5004a74>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-41-2226f5004a74>\"\u001b[1;36m, line \u001b[1;32m14\u001b[0m\n\u001b[1;33m    enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden) # 128 개 출력\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "## 신경망 model 구성 \n",
    "# seq2seq 모델은 인코더의 입력과 디코더의 형식이 같음\n",
    "# [ batch_size, time step, input size ]\n",
    "# 디코더의 출력형식\n",
    "# [ batch size, time step]\n",
    "enc_input = tf.placeholder(tf.float32, [None, None, n_input]) # 동일하게 \n",
    "# 배치크기, 글자 길이가 정해져 있지않음\n",
    "# 같은 배치인경우에는 입력데이터의 글자는 모두같아야함\n",
    "dec_input = tf.placeholder(tf.float32, [None, None, n_input]) # 동일하게 \n",
    "targets = tf.placeholder(tf.int64, [None, None])\n",
    "\n",
    "with tf.variable_scope('encoed'):\n",
    "# 인코더 셀 구성\n",
    "enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden) # 128 개 출력\n",
    "# 오버피팅 방지하기 위해 드랍아웃\n",
    "enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5) \n",
    "# 드라이버 설정(states : 다음셀로 넘어가는)\n",
    "outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input, dtype=tf.float32)\n",
    "\n",
    "with tf.variable_scope('decoded'):\n",
    "# 디코더 셀 구성\n",
    "dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "dec_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n",
    "# 드라이버 설정(\n",
    "outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, \n",
    "                                        dec_input, \n",
    "                                        initial_state = enc_states, # 초기화 \n",
    "                                        dtype = tf.float32)\n",
    "\n",
    "# s2s 모델은 인코더셀의 최종상태값이 디코더셀의 초기상태값으로 입력해줘야함 \n",
    "# initial_state = enc_states\n",
    "model = tf.layers.dense(outputs, n_class, activation=None) # fully Connected 상태로 도출 단계\n",
    "# model 에 대해서 출력하는 수행결과가 [batch size, time step, input]\n",
    "# input 가장 차원이 높은거 나와야해\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits\\\n",
    "                      (logits = model, labels = targets))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# 신경망 모델 학습 \n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "input_batch, output_batch, target_batch = make_batch([seq_data])\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    _, loss =sess.run([optimizer, cost], \n",
    "                      feed_dict={enc_input : input_batch,\n",
    "                                 dec_input : output_batch,\n",
    "                                 targets : target_batch\n",
    "    })\n",
    "    print(\"epoch:\", \"%04d\" % (epoch+1),\n",
    "         \"cost:\",\"{:.6f}\".format(loss))\n",
    "print(\"모델 완료\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 해석설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(word):\n",
    "    # 단어번역\n",
    "    # [영단어, 한단어]\n",
    "    seq_data = [word, 'P'*len(word)]\n",
    "    input_batch, output_batch, target_batch = make_batch(seq_data)\n",
    "    # [batch size, time step, input]\n",
    "    tf.argmax(model, 2)\n",
    "    # model 에 대해서 출력하는 수행결과가 [batch size, time step, input]\n",
    "    # 2번째 차원인 input 차원을 argmax로 해서 가장 확률이 높은글자를 예측갑승로\n",
    "    result = sess.run(prediction, feed_dict={\n",
    "        enc_input : input_batch,\n",
    "        dec_input : output_batch,\n",
    "        targets : target_batch\n",
    "    })\n",
    "    # result = 숫자의 인덱싱\n",
    "    decoded = [char_arr[i] for i in result[0]]\n",
    "    # 결과값이 숫자로 나옴, 숫자의 인덱스에 해당하는 문자 가져옴 \n",
    "    # 출력의 끝 \"E\"이후의 글자들 제거하고 문자열 출력\n",
    "    end = decoded.index('E')\n",
    "    translated=\"\".join(decoded[:end])\n",
    "    return translated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17, 1]]\n",
      "[[17, 1], [2, 1]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-7bdb37f83e46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'word->'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 단어\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wodr->'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wodr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 단어\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'love->'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'love'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 사랑\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loev->'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loev'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 사랑\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'abcd->'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'abcd'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# ???\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-9a91f07e81cd>\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0minput_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# [batch size, time step, input]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;31m# model 에 대해서 출력하는 수행결과가 [batch size, time step, input]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# 2번째 차원인 input 차원을 argmax로 해서 가장 확률이 높은글자를 예측갑승로\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print('word->', translate('word')) # 단어\n",
    "print('wodr->', translate('wodr')) # 단어\n",
    "print('love->', translate('love')) # 사랑\n",
    "print('loev->', translate('loev')) # 사랑\n",
    "print('abcd->', translate('abcd')) # ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - ohe 하려는 준지 \n",
    "np.eye(dic_len) # : rw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
