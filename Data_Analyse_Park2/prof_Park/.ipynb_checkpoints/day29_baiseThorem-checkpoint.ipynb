{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# day 29 \n",
    "## < bayes' theorem >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Understaing for Bayes'Theorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 이해\n",
    "\n",
    "# 홍길동 받은 메일의 80 - 스팸메일\n",
    "# 스팸 95 - 대출\n",
    "# 정상 2 - 대출\n",
    "\n",
    "# 문제) \n",
    "# 대출 이라는 단어가 제목에서 발견, 스팸확률\n",
    "\n",
    "# 풀이)\n",
    "# P(스팸 | 대출) = P(대출|스팸) * P(스팸) / P(대출)\n",
    "#                = 0.95 * 0.8 / P(대출)\n",
    "# P(A|B) = A\n",
    "# P(대출) = P(대출|스팸) P(스팸) + P(대출|정상) P(정상)\n",
    "#         = 0.95 * 0.8 + 0.02 * 0.2 = 0.764\n",
    "    \n",
    "# P(스팸|대출) = 0.95 * 0.8 / 0. 764"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 프로젝트\n",
    "# 메일 스팸/정상 분류기\n",
    "# ( + RNN  )\n",
    "\n",
    "# 1)\n",
    "# 종속적 접근 ( 문맥 )\n",
    "# 문장 => 확률\n",
    "# P('나는, 지금, 몹시, 배가, 고파요')\n",
    "# 조건부확률 베이즈 이론 \n",
    "# '나는' 다음 뭐가 나오나 \n",
    "\n",
    "# 2) \n",
    "# Count(나는 지금 몹시 배가 고파요 ) / => 10\n",
    "# Count(나는 지금 몹시 배가 ) / ========> 100\n",
    "# 검색 추천 시스템\n",
    "# 이제까지 사람들이 많이 이용한 단어 추천\n",
    "\n",
    "# 모수가 대단히 작거나 0 일수도있다\n",
    "# => Ngram 을 통해 해당 문제점 탈피 할수 있다 (UNI, BI, TRI) Gram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngram 접근방식 이용한 단어 추천\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ngram 접근방식 이용한 단어 추천\n",
    "# 앞단어 하나만 보겠다 \n",
    "# N = 1 일때, 마지막요소 참조 \n",
    "# N = 2 , EX) 고파요 | (몹시, 배가)\n",
    "\n",
    "# 한계 1) 해당 N그램과 달리 중요의미를 지닌 단어가 \n",
    "# 범위 이외에 자리한다면 해단 단어를 잡지 못하고 간다\n",
    "# N 이 작으면 작을수록 모델이 양질이라 볼수 있다 \n",
    "# Out Of Voca \n",
    "\n",
    "# 한계 2) N이 너무 작으면 문맥의 정확함을 파악하기 어려워진다  \n",
    "# EX) 너는, 나는 의미 차이 파악 불가\n",
    "# SOL) RNN - WORD2VEC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Code Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF - IDF\n",
    "# Bag of Words ( BOW )\n",
    "# : 단어들의 등장 횟수로 표현 \n",
    "# 1. 주어진 단어에 대해 고유의 인덱스를 부여한다 \n",
    "# 2. 단어의 등장 횟수 벡터생성 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'오늘은 금요일입니다 내일은 토요일입니다 다음주 화요일에는 특강이 있습니다'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "import re\n",
    "\n",
    "okt = Okt()\n",
    "token = \"오늘은 금요일입니다. 내일은 토요일입니다. 다음주 화요일에는 특강이 있습니다.\"\n",
    "# token에 저장된 문자중에서 (.)을 제거하세요\n",
    "token = re.sub(\"\\.\",\"\",token)\n",
    "token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘',\n",
       " '은',\n",
       " '금요일',\n",
       " '입니다',\n",
       " '내일',\n",
       " '은',\n",
       " '토요일',\n",
       " '입니다',\n",
       " '다음주',\n",
       " '화요일',\n",
       " '에는',\n",
       " '특강',\n",
       " '이',\n",
       " '있습니다']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.morphs(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 주어진 단어에 대해 고유의 인덱스를 부여한다 \n",
    "len(okt.morphs(token)) # (0-13)\n",
    "# '은', '입니다' 중복되므로 => 0 ~ 11 인덱싱\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 단어의 등장 횟수 벡터생성 \n",
    "# {'오늘' : 0,.....'있습니다':11}\n",
    "# [1,2,1,1,1....]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'은': 2, '입니다': 2, '오늘': 1, '금요일': 1, '내일': 1, '토요일': 1, '다음주': 1, '화요일': 1, '에는': 1, '특강': 1, ...})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "count=FreqDist(okt.morphs(token))\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('오늘은', 1),\n",
       "             ('금요일입니다', 1),\n",
       "             ('내일은', 1),\n",
       "             ('토요일입니다', 1),\n",
       "             ('다음주', 1),\n",
       "             ('화요일에는', 1),\n",
       "             ('특강이', 1),\n",
       "             ('있습니다', 1)])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from keras_preprocessing.text import Tokenizer\n",
    "# tok = Tokenizer()\n",
    "# tok.fit_on_texts([token])\n",
    "# tok.word_counts\n",
    "# tok.word_index # 인덱싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'오': 0, '늘': 1, '은': 2, ' ': 3, '금': 4, '요': 5, '일': 6, '입': 7, '니': 8, '다': 9, '내': 10, '토': 11, '음': 12, '주': 13, '화': 14, '에': 15, '는': 16, '특': 17, '강': 18, '이': 19, '있': 20, '습': 21}\n"
     ]
    }
   ],
   "source": [
    "word2index = {}\n",
    "bow = []\n",
    "for voc in token:\n",
    "    if voc not in word2index.keys():\n",
    "            word2index[voc] = len(word2index)\n",
    "            bow.insert(len(word2index)-1, 1)\n",
    "    else:\n",
    "            index = word2index.get(voc)\n",
    "            bow[index] = bow[index] + 1\n",
    "print(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 7, 1, 3, 4, 2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 2 1 2 1]]\n",
      "{'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text = ['you know I want your love. because I love you.']\n",
    "# vec = CountVectorizer()\n",
    "vec = CountVectorizer(stop_words=['|']) # 불용어 사용 (사용자정의)\n",
    "vec = CountVectorizer(stop_words=['english']) # 불용어 사용 (사용자정의)\n",
    "print(vec.fit_transform(text).toarray()) # 각 단어별 개수를 세며, 변환 \n",
    "print(vec.vocabulary_)\n",
    "# = > {'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n",
    "# 각각의 숫자는 해당 인덱스 문자를 뜻혀\n",
    "# countVectorizer = BOW 생성 클래스 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장간 유사도 조사 \n",
    "# ( 1 - target ) 오늘 역삼에서 맛있는 돈가스를 먹었다\n",
    "# ( 2 - source ) 역삼에서 먹었던 오늘의 돈가스는 맛있었다. \n",
    "\n",
    "# 2번 원문에 대해 1번 대상문장이 어느정도 유사한지 출력\n",
    "# 1번 문자에 대해 n=2 로 하여 문장 분리하면 \n",
    "# => '오늘','늘 ',' 역','역삼',...'다.'\n",
    "# => 만약 길이가 20 이라 가정하고, \n",
    "# 2번 원문에 대해서도 n = 2로 하여 문장을 분리할시, 공통으로 존재하는 단어 5개\n",
    "# 존재한다면 , 5/20 = 25% 유사도를 갖는다 \n",
    "\n",
    "# 두 문장의 유사도? (By Using N-gRAM)\n",
    "# ex) n = 2\n",
    "# (1) '오늘','늘 ',' 역','역삼',...'다.' # 두글자씩 끊는다\n",
    "# ex) 20개 중 5개가 (2)과 일치 \n",
    "# (2) '역삼','삼에','에서',...'다.'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n( 2 ) 정도로...이 커지면 커질수록 유사도는 떨어진다 \n",
    "# 토크나이즈 짜르고\n",
    "# 불용어 날리고 \n",
    "# 형태소 분리 / 명사가지고 비교\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF(단어 빈도, term frequency)는 특정한 단어가 문서 내에 \n",
    "# 얼마나 자주 등장하는지를 나타내는 값.\n",
    "# 이 값이 높을수록 문서에서 중요하다고 생각할 수 있다.\n",
    "# TF : 문서 D에서 단어 T의 등장 횟수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 1 0 1 1]\n",
      " [0 0 1 0 0 0 0 1 0]\n",
      " [1 0 0 0 1 0 1 0 0]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "corpus = ['you know I want your love',\n",
    "          'I like you',\n",
    "          'what i should i do']\n",
    "#DTM 생성\n",
    "vec=CountVectorizer()\n",
    "print(vec.fit_transform(corpus).toarray())\n",
    "print(vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "==================================================\n",
      "[[0.         0.46735098 0.         0.46735098 0.         0.46735098\n",
      "  0.         0.35543247 0.46735098]\n",
      " [0.         0.         0.79596054 0.         0.         0.\n",
      "  0.         0.60534851 0.        ]\n",
      " [0.57735027 0.         0.         0.         0.57735027 0.\n",
      "  0.57735027 0.         0.        ]]\n",
      "==================================================\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "# DTM 생성\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfv=TfidfVectorizer().fit(corpus)\n",
    "print(tfidfv)\n",
    "print('='*50)\n",
    "print(tfidfv.transform(corpus).toarray())\n",
    "print('='*50)\n",
    "print(tfidfv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n",
      "11314 11314 20\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "7\n",
      "rec.autos\n",
      "From: guykuo@carson.u.washington.edu (Guy Kuo)\n",
      "Subject: SI Clock Poll - Final Call\n",
      "Summary: Final call for SI clock reports\n",
      "Keywords: SI,acceleration,clock,upgrade\n",
      "Article-I.D.: shelley.1qvfo9INNc3s\n",
      "Organization: University of Washington\n",
      "Lines: 11\n",
      "NNTP-Posting-Host: carson.u.washington.edu\n",
      "\n",
      "A fair number of brave souls who upgraded their SI clock oscillator have\n",
      "shared their experiences for this poll. Please send a brief message detailing\n",
      "your experiences with the procedure. Top speed attained, CPU rated speed,\n",
      "add on cards and adapters, heat sinks, hour of usage per day, floppy disk\n",
      "functionality with 800 and 1.4 m floppies are especially requested.\n",
      "\n",
      "I will be summarizing in the next two days, so please add to the network\n",
      "knowledge base if you have done the clock upgrade and haven't answered this\n",
      "poll. Thanks.\n",
      "\n",
      "Guy Kuo <guykuo@u.washington.edu>\n",
      "\n",
      "4\n",
      "comp.sys.mac.hardware\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# 일종의 글이 있고 어떠한 분류에 의한 글인가 \n",
    "\n",
    "newsdata = fetch_20newsgroups(subset='train')\n",
    "# train / test data 나눠져 있다\n",
    "# 11400 개 / 7500 개 \n",
    "\n",
    "print(newsdata.keys())\n",
    "print(len(newsdata.data), len(newsdata.filenames), len(newsdata.target_names))\n",
    "print(newsdata.target_names)\n",
    "# 20 가지의 노드 ... \n",
    "print(newsdata.target[0])\n",
    "print(newsdata.target_names[7])\n",
    "print(newsdata.data[1]) # newsdata.data = 뉴스들이 담겨있따 \n",
    "# 20개의 카테고리 중에서도 rec.autos 에 해당하는 글이다\n",
    "print(newsdata.target[1]) # 이문서의 분류가 어떤 분류냐 / 분류의 번호\n",
    "print(newsdata.target_names[4]) # 분류의 이름은?\n",
    "\n",
    "# 일종의 글이 있고 어떠한 분류에 의한 글인가 \n",
    "\n",
    "# => 빈도표와 우도표의 mixture 표 도출 \n",
    "# => 사후확률이 도출되어진다\n",
    "\n",
    "# # 11314 행 - 문서, 열 - 단어\n",
    "# sklearn lib.bayes package _ a various of classifier pick! \n",
    "# fit 하면, 분류기 생성\n",
    "# 그 다음 test 한다 \n",
    "# laplace : default = 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# future image output\n",
    "# P(분류 1 | TEST DOC 1 ) = 사후확률? ===> max\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# P(분류 20 | TEST DOC 1 ) = 사후확률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 텍스트 -> bow 생성\n",
    "dtmVector = CountVectorizer()\n",
    "xtrainDtm = dtmVector.fit_transform(newsdata.data)\n",
    "xtrainDtm.shape\n",
    "# 여기등장하지않은건 oov 이다 \n",
    "# bow에 없다 \n",
    "# TF-IDF 행렬을 이용해서 분류하면 정확도 높여진다\n",
    "# DTM => TF-IDF 변환 성능 더욱 고성능화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DTM -> TF-IDF 행렬로 변환\n",
    "# 위의 TF-IDF 바로 생성 하던 클래스와 다르다\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidfv = tfidf_transformer.fit_transform(xtrainDtm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 나이브베이즈 분류기 모델 생성\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model.fit(tfidfv, newsdata.target) \n",
    "# fit 에 data 만 너겨주면 끝나 # (train data, newsdata.target) #답이 뭔지도 알려줘야해\n",
    "# alpha : raplace smooth default value\n",
    "# 모델 완성했으니 테스트 데이터넣어 테스트 레이블과 비교하여 해당 acc 도출\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = fetch_20newsgroups(subset='test',shuffle=True)\n",
    "xtestDtm = dtmVector.transform(testdata.data)\n",
    "tfidf_test = tfidf_transformer.transform(xtestDtm)\n",
    "# Dtm으로 변환 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.7738980350504514\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predicted =  model.predict(tfidf_test)\n",
    "print(\"정확도:\",accuracy_score(testdata.target, predicted))\n",
    "# 토픽 모델링  # 주제어찾기  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문제 \n",
    "\n",
    "\"\"\"\n",
    "1. 스팸필터\n",
    "-홍길동 받은 메일의 80% 스팸메일.\n",
    "-스팸메일의 95%에서 '대출'이라는 단어\n",
    "-정상메일의 2%에서 '대출' 단어\n",
    "---------주어진 데이터--------\n",
    "문제)방금 메일을 받음.\n",
    "'대출'이라는 단어가 제목에서 발견됨.\n",
    "스팸일 확률?\n",
    "P(스팸|대출)=P(대출|스팸)P(스팸) / P(대출)\n",
    "                = 0.95*0.8 / P(대출)\n",
    "\n",
    "P(대출)=P(대출|스팸)P(스팸)+P(대출|정상)P(정상)\n",
    "=             0.95 * 0.8 + 0.02 * 0.2 = 0.764\n",
    "\n",
    "P(스팸|대출)= 0.95*0.8 / 0.764 =???\n",
    "\n",
    "\n",
    "2. 문장간 유사도 조사\n",
    "\n",
    "(1, target) \n",
    "오늘 역삼에서 맛있는 돈가스를 먹었다.\n",
    "(2, source) \n",
    "역삼에서 먹었던 오늘의 돈가스는 맛있었다.\n",
    "\n",
    "\n",
    "(2)번 원문장에 대해 (1)번 대상문장이 어느정도 유사한지 출력?\n",
    "(1)번 문장에 대해 n=2로 하여 문장 분리하면\n",
    "=> '오늘','늘 ',' 역','역삼', ... '다.'\n",
    "=>만약 길이가 20이라 가정하고, \n",
    "(2)번 원문장에 대해서도 n=2로 하여 문장 분리했을때,\n",
    "공통으로 존재하는 단어가 5개 존재한다면,\n",
    "5/20 = 25%의 유사도를 갖는다.\n",
    "\n",
    "두 문장의 유사도?(using n-gram)\n",
    "ex) n=2\n",
    "(1) '오늘','늘 ',' 역','역삼', ... '다.'  \n",
    "=> 20개 중 5개가 (2)과 일치\n",
    "(2) '역삼', '삼에', '에서', ...'다.'\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
