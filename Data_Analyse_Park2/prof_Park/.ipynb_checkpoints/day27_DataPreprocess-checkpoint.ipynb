{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# day 27\n",
    "\n",
    "# tensorfloew _ keras \n",
    "\n",
    "from keras.datasets import mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 가져오기 \n",
    "# 함수 train test 각 각 로딩\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images.shape # (60000, 28, 28)\n",
    "test_images.shape # (10000, 28, 28)\n",
    "train_labels\n",
    "# test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0807 15:31:06.380115 18504 deprecation_wrapper.py:119] From C:\\Users\\user\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0807 15:31:06.396071 18504 deprecation_wrapper.py:119] From C:\\Users\\user\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0807 15:31:06.398066 18504 deprecation_wrapper.py:119] From C:\\Users\\user\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network = models.Sequential() # seq 객체생성 # 즉 , 하나의 신경망 \n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28*28,)))\n",
    "# 원래 이미지 28 * 28 => 60000, 784\n",
    "# 계층이 추가되며 입력 출력 이 어떻게 되고 활성화 함수 뭘 쓸지 명시해줘야한다 \n",
    "network.add(layers.Dense(10, activation='softmax', input_shape=(512,))) # 출력부 # input은 첫 입력계층에서만 선언해주면된다 \n",
    "# tensorflow 에서는 w x b 등등 써줘야했었음\n",
    "# keras - sub package .. dense 계층 1 개 생성 \n",
    "# Dense ( 출력 , 활성화함수 , 입력계층 )\n",
    "# 간단하게 tf version를 간소화시키는 방식을 취한다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0807 15:31:06.439955 18504 deprecation_wrapper.py:119] From C:\\Users\\user\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0807 15:31:06.461896 18504 deprecation_wrapper.py:119] From C:\\Users\\user\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 손실함수 ( loss ( cost func ))\n",
    "# opt 과정 \n",
    "# 훈련 complie\n",
    "network.compile(loss='categorical_crossentropy', optimizer = 'rmsprop', \n",
    "                metrics=['accuracy']) \n",
    "# metrics : 기준으로 잡는 것 ( 주로, 정확도 ) # [ ] 는 여러가지 항목 포함가능 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt : 모델 업데이트 (최적화)\n",
    "# cost를 최소화하는 방향으로 파라미터 업데이트하는것 \n",
    "\n",
    "# 1) 경사하강법 ( SGD ) \n",
    "# : 기울기 * 학습률(lr) # step size (보폭률) => 가중치 갱신\n",
    "## 기울어진 방향으로 만 이동하므로, 효율이낮고 시간도 많이 걸리고 복잡한 모델에 적합치 않아 \n",
    "# 2) 모멘텀 : 운동량, 속도 크면 => 기울기 크게 업데이트 [보강하는 접근방식] # 운동량을고려하자\n",
    "# 3) AdaGrad(RMSProp : 새로운 기울기만 학습률애 반영) :  갱신 할 때 제곱값이 들어간다 \n",
    "## 모멘텀의 경우, 학습률이 떨어져 개선을 시킨 버젼 \n",
    "# 4) Adam(모멘텀+AdaGrad) : 학습속도, 갱신과정 업데이트 하자! [보통, 가장 성능이 좋다]\n",
    "\n",
    "### => 학습률을 동일하게 주지말자! \n",
    "### 기울기가 다르면 학습률이 동일할 필요없다 \n",
    "### update사항에 기울기도 포함시키자 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화작업\n",
    "train_images.shape\n",
    "train_images = train_images.reshape((60000, 28*28))\n",
    "train_images = train_images.astype('float32')/255 # int - > float\n",
    "## 모델 정규화 ( 0 - 1 사이 ) < - ( 0 - 255 ) 데이터를 // 변환하고 싶을시 선언\n",
    "test_images.shape\n",
    "test_images = test_images.reshape((10000, 28*28))\n",
    "test_images = test_images.astype('float32')/255\n",
    "## 모델을 만들게 되면 28 * 28 = 784\n",
    "## 512 입력(relu), 10개로 출력 (softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "# ohe 해줘야한다 \n",
    "train_labels = to_categorical(train_labels) # 범주형변환 \n",
    "test_labels = to_categorical(test_labels)\n",
    "# test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0807 15:31:07.050890 18504 deprecation.py:323] From C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0807 15:31:07.111705 18504 deprecation_wrapper.py:119] From C:\\Users\\user\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.2623 - acc: 0.9242\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.1053 - acc: 0.9683\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.0689 - acc: 0.9791\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.0505 - acc: 0.9845\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.0374 - acc: 0.9889\n",
      "10000/10000 [==============================] - 0s 43us/step\n"
     ]
    }
   ],
   "source": [
    "network.fit(train_images, train_labels, epochs = 5, batch_size = 128 ) #전체 데이터 train 5\n",
    "# network 이용, evaluate => test 실행 \n",
    "test_cost, test_acc = network.evaluate(test_images, test_labels)\n",
    "## 결과 -- lost(cost) 점점 떨어지고, acc(accuracy) 는 계속 올라간다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9763\n"
     ]
    }
   ],
   "source": [
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 처리 ( 자연어 처리 ) => RNN \n",
    "# 영상처리 RNN 애서도 움직이는 영상처리 와 활용 \n",
    "# 여러장의 이미지가 입력됬을 떄 어떤 동작하는가\n",
    "# ex ) 침대에 떨어질것같은 사람의 동작 모습 이미지를 인식하고\n",
    "# 이를 기반으로 결과를 예측하여 이에 대한 sol 제공 ( smart home )\n",
    "# CNN + RNN(Deep Learning's flower)\n",
    "# (Deep Learning's daddy, hinton)\n",
    "# (Deep Learning's daddy, walking uncle, park gil sik)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download() # 단어 인식을 위한 사전 가져오기 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', ',', 'touch', ',', 'me']\n"
     ]
    }
   ],
   "source": [
    "# from konlpy.tag import Twitter # 기본적으로 자바로 만들어져있어\n",
    "# 한글 사용 패키지 설치 필요 -> jaypipe\n",
    "import nltk # 영문 사용 패키지 \n",
    "# 한글 konlpy -> jay pipe\n",
    "from nltk.tokenize import word_tokenize\n",
    "# print(word_tokenize('How are you?'))\n",
    "\n",
    "# print(word_tokenize(\"Don't touch me\"))\n",
    "# output : ['Do', \"n't\", 'touch', 'me']\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "# print(word_tokenize(\"Don't touch me\"))\n",
    "# ['Do', \"n't\", 'touch', 'me']\n",
    "\n",
    "print(WordPunctTokenizer().tokenize(\"Don't, touch, me\"))\n",
    "# tokenize : 객체로 만들어줘\n",
    "# tokenizer 마다 처리 결과가 조금씩 다르다 \n",
    "# n't => not 으로 변환시켜줘야함 \n",
    "\n",
    "# corpus : 관심을 가지고 있는 영역, 도메인에 대한 단어들의 묶음, 모음\n",
    "# corpus 구축 --- 중요단계\n",
    "# corpus () : token 단위로 나누는 작업수행 \n",
    "# 단어를 단어단위로 나눈다 .. 단어 토큰화\n",
    "# 문자단위 나눈다 .. 문자 토큰화\n",
    "# 여러 토큰 단위 나눔작업하는것이 tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\jpype\\_core.py:210: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('아버지', 'Noun'),\n",
       " ('가', 'Josa'),\n",
       " ('방', 'Noun'),\n",
       " ('에', 'Josa'),\n",
       " ('들어가신다', 'Verb')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "# okt.pos('아버지 가방에 들어가신다')\n",
    "okt.pos('아버지가 방에 들어가신다')\n",
    "# 자료결과 자료형 리스트 []\n",
    "# 자료결과 내부 튜플 ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[('롯데마트', 'ncn'), ('의', 'jcm')], [('롯데마트의', 'ncn')], [('롯데마트', 'nqq'), ('의', 'jcm')], [('롯데마트의', 'nqq')]], [[('흑마늘', 'ncn')], [('흑마늘', 'nqq')]], [[('양념', 'ncn')]], [[('치킨', 'ncn'), ('이', 'jcc')], [('치킨', 'ncn'), ('이', 'jcs')], [('치킨', 'ncn'), ('이', 'ncn')]], [[('논란', 'ncpa'), ('이', 'jcc')], [('논란', 'ncpa'), ('이', 'jcs')], [('논란', 'ncpa'), ('이', 'ncn')]], [[('되', 'nbu'), ('고', 'jcj')], [('되', 'nbu'), ('이', 'jp'), ('고', 'ecc')], [('되', 'nbu'), ('이', 'jp'), ('고', 'ecs')], [('되', 'nbu'), ('이', 'jp'), ('고', 'ecx')], [('되', 'paa'), ('고', 'ecc')], [('되', 'paa'), ('고', 'ecs')], [('되', 'paa'), ('고', 'ecx')], [('되', 'pvg'), ('고', 'ecc')], [('되', 'pvg'), ('고', 'ecs')], [('되', 'pvg'), ('고', 'ecx')], [('되', 'px'), ('고', 'ecc')], [('되', 'px'), ('고', 'ecs')], [('되', 'px'), ('고', 'ecx')]], [[('있', 'paa'), ('다', 'ef')], [('있', 'px'), ('다', 'ef')]], [[('.', 'sf')], [('.', 'sy')]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[[('롯데마트', 'ncn'), ('의', 'jcm')],\n",
       "  [('롯데마트의', 'ncn')],\n",
       "  [('롯데마트', 'nqq'), ('의', 'jcm')],\n",
       "  [('롯데마트의', 'nqq')]],\n",
       " [[('흑마늘', 'ncn')], [('흑마늘', 'nqq')]],\n",
       " [[('양념', 'ncn')]],\n",
       " [[('치킨', 'ncn'), ('이', 'jcc')],\n",
       "  [('치킨', 'ncn'), ('이', 'jcs')],\n",
       "  [('치킨', 'ncn'), ('이', 'ncn')]],\n",
       " [[('논란', 'ncpa'), ('이', 'jcc')],\n",
       "  [('논란', 'ncpa'), ('이', 'jcs')],\n",
       "  [('논란', 'ncpa'), ('이', 'ncn')]],\n",
       " [[('되', 'nbu'), ('고', 'jcj')],\n",
       "  [('되', 'nbu'), ('이', 'jp'), ('고', 'ecc')],\n",
       "  [('되', 'nbu'), ('이', 'jp'), ('고', 'ecs')],\n",
       "  [('되', 'nbu'), ('이', 'jp'), ('고', 'ecx')],\n",
       "  [('되', 'paa'), ('고', 'ecc')],\n",
       "  [('되', 'paa'), ('고', 'ecs')],\n",
       "  [('되', 'paa'), ('고', 'ecx')],\n",
       "  [('되', 'pvg'), ('고', 'ecc')],\n",
       "  [('되', 'pvg'), ('고', 'ecs')],\n",
       "  [('되', 'pvg'), ('고', 'ecx')],\n",
       "  [('되', 'px'), ('고', 'ecc')],\n",
       "  [('되', 'px'), ('고', 'ecs')],\n",
       "  [('되', 'px'), ('고', 'ecx')]],\n",
       " [[('있', 'paa'), ('다', 'ef')], [('있', 'px'), ('다', 'ef')]],\n",
       " [[('.', 'sf')], [('.', 'sy')]]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한나눔 클래스 \n",
    "from konlpy.tag import Hannanum\n",
    "hannanum = Hannanum()\n",
    "print(hannanum.analyze(u'롯데마트의 흑마늘 양념 치킨이 논란이 되고 있다.'))\n",
    "[[[('롯데마트', 'ncn'), ('의', 'jcm')], [('롯데마트의', 'ncn')], [('롯데마트', 'nqq'), ('의', 'jcm')], [('롯데마트의', 'nqq')]], [[('흑마늘', 'ncn')], [('흑마늘', 'nqq')]], [[('양념', 'ncn')]], [[('치킨', 'ncn'), ('이', 'jcc')], [('치킨', 'ncn'), ('이', 'jcs')], [('치킨', 'ncn'), ('이', 'ncn')]], [[('논란', 'ncpa'), ('이', 'jcc')], [('논란', 'ncpa'), ('이', 'jcs')], [('논란', 'ncpa'), ('이', 'ncn')]], [[('되', 'nbu'), ('고', 'jcj')], [('되', 'nbu'), ('이', 'jp'), ('고', 'ecc')], [('되', 'nbu'), ('이', 'jp'), ('고', 'ecs')], [('되', 'nbu'), ('이', 'jp'), ('고', 'ecx')], [('되', 'paa'), ('고', 'ecc')], [('되', 'paa'), ('고', 'ecs')], [('되', 'paa'), ('고', 'ecx')], [('되', 'pvg'), ('고', 'ecc')], [('되', 'pvg'), ('고', 'ecs')], [('되', 'pvg'), ('고', 'ecx')], [('되', 'px'), ('고', 'ecc')], [('되', 'px'), ('고', 'ecs')], [('되', 'px'), ('고', 'ecx')]], [[('있', 'paa'), ('다', 'ef')], [('있', 'px'), ('다', 'ef')]], [[('.', 'sf')], [('.', 'sy')]]]\n",
    "# 형태소 단위로 나누어져 있어여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# korean POS tags comparison chart\n",
    "# twitter Korean Text Column ref (명사 동사 주요 추출 대상)\n",
    "# https://docs.google.com/spreadsheets/d/1OGAjUvalBuX-oZvZ_-9tEfYD2gQe7hTGsgUpiiBSXI8/edit#gid=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************************\n",
      "['Python is an interpreted, high-level, general-purpose programming language.', \"Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\", 'Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.', '[27] Python is dynamically typed and garbage-collected.', 'It supports multiple programming paradigms, including procedural, object-oriented, and functional programming.', \"Python is often described as a 'batteries included' language due to its comprehensive standard library.\", '[28]']\n",
      "************************************************************************************\n",
      "['Python', 'is', 'an', 'interpreted', ',', 'high-level', ',', 'general-purpose', 'programming', 'language', '.', 'Created', 'by', 'Guido', 'van', 'Rossum', 'and', 'first', 'released', 'in', '1991', ',', 'Python', \"'s\", 'design', 'philosophy', 'emphasizes', 'code', 'readability', 'with', 'its', 'notable', 'use', 'of', 'significant', 'whitespace', '.', 'Its', 'language', 'constructs', 'and', 'object-oriented', 'approach', 'aim', 'to', 'help', 'programmers', 'write', 'clear', ',', 'logical', 'code', 'for', 'small', 'and', 'large-scale', 'projects', '.', '[', '27', ']', 'Python', 'is', 'dynamically', 'typed', 'and', 'garbage-collected', '.', 'It', 'supports', 'multiple', 'programming', 'paradigms', ',', 'including', 'procedural', ',', 'object-oriented', ',', 'and', 'functional', 'programming', '.', 'Python', 'is', 'often', 'described', 'as', 'a', \"'batteries\", 'included', \"'\", 'language', 'due', 'to', 'its', 'comprehensive', 'standard', 'library', '.', '[', '28', ']']\n",
      "************************************************************************************\n",
      "[('Python', 'NNP'), ('is', 'VBZ'), ('an', 'DT'), ('interpreted', 'JJ'), (',', ','), ('high-level', 'JJ'), (',', ','), ('general-purpose', 'JJ'), ('programming', 'NN'), ('language', 'NN'), ('.', '.'), ('Created', 'VBN'), ('by', 'IN'), ('Guido', 'NNP'), ('van', 'NN'), ('Rossum', 'NNP'), ('and', 'CC'), ('first', 'RB'), ('released', 'VBN'), ('in', 'IN'), ('1991', 'CD'), (',', ','), ('Python', 'NNP'), (\"'s\", 'POS'), ('design', 'NN'), ('philosophy', 'NN'), ('emphasizes', 'VBZ'), ('code', 'JJ'), ('readability', 'NN'), ('with', 'IN'), ('its', 'PRP$'), ('notable', 'JJ'), ('use', 'NN'), ('of', 'IN'), ('significant', 'JJ'), ('whitespace', 'NN'), ('.', '.'), ('Its', 'PRP$'), ('language', 'NN'), ('constructs', 'NNS'), ('and', 'CC'), ('object-oriented', 'JJ'), ('approach', 'NN'), ('aim', 'NN'), ('to', 'TO'), ('help', 'VB'), ('programmers', 'NNS'), ('write', 'VB'), ('clear', 'JJ'), (',', ','), ('logical', 'JJ'), ('code', 'NN'), ('for', 'IN'), ('small', 'JJ'), ('and', 'CC'), ('large-scale', 'JJ'), ('projects', 'NNS'), ('.', '.'), ('[', '$'), ('27', 'CD'), (']', 'NNP'), ('Python', 'NNP'), ('is', 'VBZ'), ('dynamically', 'RB'), ('typed', 'JJ'), ('and', 'CC'), ('garbage-collected', 'JJ'), ('.', '.'), ('It', 'PRP'), ('supports', 'VBZ'), ('multiple', 'JJ'), ('programming', 'NN'), ('paradigms', 'NN'), (',', ','), ('including', 'VBG'), ('procedural', 'JJ'), (',', ','), ('object-oriented', 'JJ'), (',', ','), ('and', 'CC'), ('functional', 'JJ'), ('programming', 'NN'), ('.', '.'), ('Python', 'NNP'), ('is', 'VBZ'), ('often', 'RB'), ('described', 'VBN'), ('as', 'IN'), ('a', 'DT'), (\"'batteries\", 'NNS'), ('included', 'VBD'), (\"'\", 'POS'), ('language', 'NN'), ('due', 'JJ'), ('to', 'TO'), ('its', 'PRP$'), ('comprehensive', 'JJ'), ('standard', 'NN'), ('library', 'NN'), ('.', '.'), ('[', 'CC'), ('28', 'CD'), (']', 'NN')]\n",
      "************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import *\n",
    "from nltk.tag import *\n",
    "text = \"Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.[27] Python is dynamically typed and garbage-collected. It supports multiple programming paradigms, including procedural, object-oriented, and functional programming. Python is often described as a 'batteries included' language due to its comprehensive standard library.[28]\"\n",
    "print('*'*84)\n",
    "print(sent_tokenize(text))\n",
    "print('*'*84)\n",
    "print(word_tokenize(text))\n",
    "print('*'*84)\n",
    "print(pos_tag(word_tokenize(text))) # 품사 단위 token\n",
    "print('*'*84) # 인칭 대명사 # 지시 대명사들 detail 하게 분류되어져있는 사전\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************************\n",
      "['오늘', '도', '지각', '하지', '않고', '열심히', '공부', '한', '여러분', ',', '이번', '주도', '힘냅시다', '.']\n",
      "************************************************************************************\n",
      "['오늘', '도', '지각', '하', '지', '않', '고', '열심히', '공부', '하', 'ㄴ', '여러분', ',', '이번', '주도', '힘내', 'ㅂ시다', '.']\n",
      "************************************************************************************\n",
      "[('오늘', 'Noun'), ('도', 'Josa'), ('지각', 'Noun'), ('하지', 'Verb'), ('않고', 'Verb'), ('열심히', 'Adverb'), ('공부', 'Noun'), ('한', 'Josa'), ('여러분', 'Noun'), (',', 'Punctuation'), ('이번', 'Noun'), ('주도', 'Noun'), ('힘냅시다', 'Verb'), ('.', 'Punctuation')]\n",
      "************************************************************************************\n",
      "['오늘', '지각', '공부', '여러분', '이번', '주도']\n",
      "************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "print('*'*84)\n",
    "print(okt.morphs(\"오늘도 지각하지 않고 열심히 공부한 여러분, 이번주도 힘냅시다.\"))\n",
    "print('*'*84)\n",
    "# morphs : 형태도 단위 분류\n",
    "from konlpy.tag import Kkma\n",
    "kma = Kkma() # 꼬꼬마 객체 class\n",
    "# 하지 vs 하, 지\n",
    "\n",
    "print(kma.morphs(\"오늘도 지각하지 않고 열심히 공부한 여러분, 이번주도 힘냅시다.\"))\n",
    "print('*'*84)\n",
    "print(okt.pos(\"오늘도 지각하지 않고 열심히 공부한 여러분, 이번주도 힘냅시다.\"))\n",
    "print('*'*84)\n",
    "print(okt.nouns(\"오늘도 지각하지 않고 열심히 공부한 여러분, 이번주도 힘냅시다.\"))\n",
    "print('*'*84) # 명사만 추출\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대소문자 통합, 불용어 제거 ( 대체 ), ( ' ) 특수문자 처리\n",
    "# 영어 단어 _ 시제, \n",
    "# ex) train, training, trains = 같은 단어로 취급하고 통일 처리해줘야한다\n",
    "# ex) are/is/was/were => be 동사\n",
    "# 정규표현식 적절한 사용 \n",
    "\n",
    "# 형태소 분석기 다루는중 ,,,,\n",
    "# 어간 ( stem ), 접사\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have\n",
      "be\n",
      "be\n"
     ]
    }
   ],
   "source": [
    "# 인코딩 \n",
    "# 불용어에 있는 단어는 제거하고 작업을 진행해야했다..\n",
    "\n",
    "# 단어 통일시에 유용한 라이브러리 \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "print(wnl.lemmatize('has','v')) \n",
    "print(wnl.lemmatize('were','v'))\n",
    "print(wnl.lemmatize('was','v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************************\n",
      "['Python', 'is', 'an', 'interpreted', ',', 'high-level', ',', 'general-purpose', 'programming', 'language', '.', 'Created', 'by', 'Guido', 'van', 'Rossum', 'and', 'first', 'released', 'in', '1991', ',', 'Python', \"'s\", 'design', 'philosophy', 'emphasizes', 'code', 'readability', 'with', 'its', 'notable', 'use', 'of', 'significant', 'whitespace', '.', 'Its', 'language', 'constructs', 'and', 'object-oriented', 'approach', 'aim', 'to', 'help', 'programmers', 'write', 'clear', ',', 'logical', 'code', 'for', 'small', 'and', 'large-scale', 'projects', '.', '[', '27', ']', 'Python', 'is', 'dynamically', 'typed', 'and', 'garbage-collected', '.', 'It', 'supports', 'multiple', 'programming', 'paradigms', ',', 'including', 'procedural', ',', 'object-oriented', ',', 'and', 'functional', 'programming', '.', 'Python', 'is', 'often', 'described', 'as', 'a', \"'batteries\", 'included', \"'\", 'language', 'due', 'to', 'its', 'comprehensive', 'standard', 'library', '.', '[', '28', ']']\n",
      "************************************************************************************\n",
      "['python', 'is', 'an', 'interpret', ',', 'high-level', ',', 'general-purpos', 'program', 'languag', '.', 'creat', 'by', 'guido', 'van', 'rossum', 'and', 'first', 'releas', 'in', '1991', ',', 'python', \"'s\", 'design', 'philosophi', 'emphas', 'code', 'readabl', 'with', 'it', 'notabl', 'use', 'of', 'signific', 'whitespac', '.', 'it', 'languag', 'construct', 'and', 'object-ori', 'approach', 'aim', 'to', 'help', 'programm', 'write', 'clear', ',', 'logic', 'code', 'for', 'small', 'and', 'large-scal', 'project', '.', '[', '27', ']', 'python', 'is', 'dynam', 'type', 'and', 'garbage-collect', '.', 'It', 'support', 'multipl', 'program', 'paradigm', ',', 'includ', 'procedur', ',', 'object-ori', ',', 'and', 'function', 'program', '.', 'python', 'is', 'often', 'describ', 'as', 'a', \"'batteri\", 'includ', \"'\", 'languag', 'due', 'to', 'it', 'comprehens', 'standard', 'librari', '.', '[', '28', ']']\n",
      "(1)*********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# 포터 알고리즘 # 어간 추출 \n",
    "from nltk.stem import PorterStemmer\n",
    "# 클래스니깐 객체 생성해줘야한다\n",
    "print('*'*84)\n",
    "ps = PorterStemmer()\n",
    "words = word_tokenize(text)\n",
    "print(words)\n",
    "print('*'*84)\n",
    "\n",
    "ps_words = [ps.stem(w) for w in words] # w읽어서 stem함수 .. c추출 된걸 읽어라\n",
    "print(ps_words) # 어간 추출\n",
    "print('(1)'+'*' * 81) # exer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['need', 'help', '.', 'like', 'coding', '.', \"'s\", 'hobby', '.']\n"
     ]
    }
   ],
   "source": [
    "# 어미\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english') # 불용어 의미하는데 크게 영향력이 없다 \n",
    "# sw\n",
    "test = \"i need you to help me. i like coding. what's your hobby.\"\n",
    "test = word_tokenize(test) # 단어 토큰화\n",
    "test\n",
    "# 불용어 빼고 출력하겠다\n",
    "res = []\n",
    "for w in test:\n",
    "    if w not in sw:\n",
    "        res.append(w)\n",
    "print(res) # 의미해서에 필요한 단어만 추출되어졌다(불용어제거)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['파이썬', '코딩을', '해야합니다', '.', '하기싫어도', '해요', '.', '그래도', '하세요']\n"
     ]
    }
   ],
   "source": [
    "sw = ' 열심히 하기싫어 싫거든 안해 '\n",
    "test = \"\"\" \n",
    "파이썬 코딩을 열심히 해야합니다. 하기싫어도 해요.\n",
    "그래도 싫거든 하세요 안해 안해 안해\n",
    "\"\"\"\n",
    "\n",
    "sw = sw.split(\" \") # 단어단위 나눠 // 공백기준 단어추출\n",
    "sw # 불용어 \n",
    "test = word_tokenize(test) # . 과 같은것들이 분리되고 \n",
    "res2 = []\n",
    "for w in test:\n",
    "    if w not in sw:\n",
    "        res2.append(w)\n",
    "print(res2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tip ) 자연어 처리 잘하려면 정규표현식을 잘 하세요~\n",
    "# 베이즈 이론 - 조건부확률 - 확률부 접근 \n",
    "# ex ) 비가 온다 (조건)________천둥칠 확률은 ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'python': 3, 'is': 3, 'an': 1, 'interpreted': 1, 'high-level': 1, 'general-purpose': 1, 'programming': 3, 'language': 3, 'created': 1, 'by': 1, 'guido': 1, 'van': 1, 'rossum': 1, 'and': 5, 'first': 1, 'released': 1, 'in': 1, '1991': 1, \"python's\": 1, 'design': 1, 'philosophy': 1, 'emphasizes': 1, 'code': 2, 'readability': 1, 'with': 1, 'its': 3, 'notable': 1, 'use': 1, 'of': 1, 'significant': 1, 'whitespace': 1, 'constructs': 1, 'object-oriented': 2, 'approach': 1, 'aim': 1, 'to': 2, 'help': 1, 'programmers': 1, 'write': 1, 'clear': 1, 'logical': 1, 'for': 1, 'small': 1, 'large-scale': 1, 'projects[27]': 1, 'dynamically': 1, 'typed': 1, 'garbage-collected': 1, 'it': 1, 'supports': 1, 'multiple': 1, 'paradigms': 1, 'including': 1, 'procedural': 1, 'functional': 1, 'often': 1, 'described': 1, 'as': 1, 'a': 1, \"'batteries\": 1, \"included'\": 1, 'due': 1, 'comprehensive': 1, 'standard': 1, 'library[28]': 1}\n"
     ]
    }
   ],
   "source": [
    "# 정수 인코딩\n",
    "# 정수인코딩이전에 \n",
    "\n",
    "# 각 단어 몇번 나왔는지 확인 출력\n",
    "# 단어 길이가 2이하인 경우 제외\n",
    "# 불용어 사전 단어 제거\n",
    "# 대소문자 구분 없음 ( 모두 소문자로 변환 ) # lower()\n",
    "# 'HAVE'.lower()\n",
    "\n",
    "##################################### exer1 ##########################################\n",
    "\n",
    "str = text.lower()\n",
    "str = str.replace(',','')\n",
    "str = str.replace('.','')\n",
    "# print(str)\n",
    "\n",
    "word_list = str.split()\n",
    "word_list\n",
    "\n",
    "word = {}\n",
    "word_count = {}\n",
    "for each in word_list:\n",
    "    if each in word_count:\n",
    "        word_count[each]+=1\n",
    "    else:\n",
    "        word_count[each]=1\n",
    "print(word_count)\n",
    "\n",
    "###################################### exer2 ############################################\n",
    "# text\n",
    "# test = word_tokenize(text) # 단어 토큰화\n",
    "# test\n",
    "\n",
    "# # 불용어 빼고 출력하겠다\n",
    "# res = []\n",
    "# for w in test:\n",
    "#     if w not in sw:\n",
    "#         res.append(w)\n",
    "# print(res) # 의미해서에 필요한 단어만 추출되어졌다(불용어제거)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "']'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english') # 불용어 의미하는데 크게 영향력이 없다 \n",
    "# sw\n",
    "test = \"i need you to help me. i like coding. what's your hobby.\"\n",
    "test = word_tokenize(test) # 단어 토큰화\n",
    "test\n",
    "# 불용어 빼고 출력하겠다\n",
    "res = []\n",
    "for w in test:\n",
    "    if w not in sw:\n",
    "        res.append(w)\n",
    "print(res) # 의미해서에 필요한 단어만 추출되어졌다(불용어제거)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.[27] Python is dynamically typed and garbage-collected. It supports multiple programming paradigms, including procedural, object-oriented, and functional programming. Python is often described as a 'batteries included' language due to its comprehensive standard library.[28]\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python',\n",
       " 'is',\n",
       " 'an',\n",
       " 'interpreted',\n",
       " ',',\n",
       " 'high-level',\n",
       " ',',\n",
       " 'general-purpose',\n",
       " 'programming',\n",
       " 'language',\n",
       " '.',\n",
       " 'Created',\n",
       " 'by',\n",
       " 'Guido',\n",
       " 'van',\n",
       " 'Rossum',\n",
       " 'and',\n",
       " 'first',\n",
       " 'released',\n",
       " 'in',\n",
       " '1991',\n",
       " ',',\n",
       " 'Python',\n",
       " \"'s\",\n",
       " 'design',\n",
       " 'philosophy',\n",
       " 'emphasizes',\n",
       " 'code',\n",
       " 'readability',\n",
       " 'with',\n",
       " 'its',\n",
       " 'notable',\n",
       " 'use',\n",
       " 'of',\n",
       " 'significant',\n",
       " 'whitespace',\n",
       " '.',\n",
       " 'Its',\n",
       " 'language',\n",
       " 'constructs',\n",
       " 'and',\n",
       " 'object-oriented',\n",
       " 'approach',\n",
       " 'aim',\n",
       " 'to',\n",
       " 'help',\n",
       " 'programmers',\n",
       " 'write',\n",
       " 'clear',\n",
       " ',',\n",
       " 'logical',\n",
       " 'code',\n",
       " 'for',\n",
       " 'small',\n",
       " 'and',\n",
       " 'large-scale',\n",
       " 'projects',\n",
       " '.',\n",
       " '[',\n",
       " '27',\n",
       " ']',\n",
       " 'Python',\n",
       " 'is',\n",
       " 'dynamically',\n",
       " 'typed',\n",
       " 'and',\n",
       " 'garbage-collected',\n",
       " '.',\n",
       " 'It',\n",
       " 'supports',\n",
       " 'multiple',\n",
       " 'programming',\n",
       " 'paradigms',\n",
       " ',',\n",
       " 'including',\n",
       " 'procedural',\n",
       " ',',\n",
       " 'object-oriented',\n",
       " ',',\n",
       " 'and',\n",
       " 'functional',\n",
       " 'programming',\n",
       " '.',\n",
       " 'Python',\n",
       " 'is',\n",
       " 'often',\n",
       " 'described',\n",
       " 'as',\n",
       " 'a',\n",
       " \"'batteries\",\n",
       " 'included',\n",
       " \"'\",\n",
       " 'language',\n",
       " 'due',\n",
       " 'to',\n",
       " 'its',\n",
       " 'comprehensive',\n",
       " 'standard',\n",
       " 'library',\n",
       " '.',\n",
       " '[',\n",
       " '28',\n",
       " ']']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "']'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python': 4,\n",
       " 'interpreted': 1,\n",
       " 'high-level': 1,\n",
       " 'general-purpose': 1,\n",
       " 'programming': 3,\n",
       " 'language': 3,\n",
       " 'created': 1,\n",
       " 'guido': 1,\n",
       " 'van': 1,\n",
       " 'rossum': 1,\n",
       " 'and': 5,\n",
       " 'first': 1,\n",
       " 'released': 1,\n",
       " '1991': 1,\n",
       " 'design': 1,\n",
       " 'philosophy': 1,\n",
       " 'emphasizes': 1,\n",
       " 'code': 2,\n",
       " 'readability': 1,\n",
       " 'with': 1,\n",
       " 'its': 3,\n",
       " 'notable': 1,\n",
       " 'use': 1,\n",
       " 'significant': 1,\n",
       " 'whitespace': 1,\n",
       " 'constructs': 1,\n",
       " 'object-oriented': 2,\n",
       " 'approach': 1,\n",
       " 'aim': 1,\n",
       " 'help': 1,\n",
       " 'programmers': 1,\n",
       " 'write': 1,\n",
       " 'clear': 1,\n",
       " 'logical': 1,\n",
       " 'for': 1,\n",
       " 'small': 1,\n",
       " 'large-scale': 1,\n",
       " 'projects': 1,\n",
       " 'dynamically': 1,\n",
       " 'typed': 1,\n",
       " 'garbage-collected': 1,\n",
       " 'supports': 1,\n",
       " 'multiple': 1,\n",
       " 'paradigms': 1,\n",
       " 'including': 1,\n",
       " 'procedural': 1,\n",
       " 'functional': 1,\n",
       " 'often': 1,\n",
       " 'described': 1,\n",
       " \"'batteries\": 1,\n",
       " 'included': 1,\n",
       " 'due': 1,\n",
       " 'comprehensive': 1,\n",
       " 'standard': 1,\n",
       " 'library': 1}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Counter\n",
    "# Counter 는 collections 아래에 정의된 사전(dict)의 서브 클래스로\n",
    "# 일련의 집합에서 각 원소의 출현 횟수를 세어서 유지한다. \n",
    "# 즉 원소:출현빈도의 사전이 생성된다. 생성된 카운터 사전은 \n",
    "# 일반 사전과 유사하게 키:값 쌍을 추가/삭제하거나 업데이트 할 수 있고, \n",
    "# 각 원소를 출현횟수만큼 반복하여 조합해서 복구한다.\n",
    "\n",
    "## 목표: 단어수\n",
    "# 소문자로만\n",
    "# 길이 3부터 \n",
    "# 파이썬 파트7. for in 반복문, range, enumerate\n",
    "# index\n",
    "# sw = stopwords.words('english')\n",
    "# 불용어 사전 단어 제거 \n",
    "text = text.lower()\n",
    "tokens = word_tokenize(text)\n",
    "dic = {}\n",
    "for token in tokens:\n",
    "    if len(token)>=3:\n",
    "        if token not in dic:\n",
    "            dic[token] = 1\n",
    "        else:\n",
    "            dic[token] += 1\n",
    "dic\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1, 'b': 2}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic={}\n",
    "dic['a']=1\n",
    "dic['b']=2\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw = stopwords.words('english')\n",
    "sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw = stopwords.words('english')\n",
    "# 불용어 사전 단어 제거 \n",
    "text = text.lower()\n",
    "tokens = word_tokenize(text)\n",
    "dic = {}\n",
    "for token in tokens:\n",
    "    if token not in sw:\n",
    "        if len(token)>=3:\n",
    "            if token not in dic:\n",
    "                dic[token] = 1\n",
    "            else:\n",
    "                dic[token] += 1\n",
    "len(dic)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 p\n",
      "1 y\n",
      "2 t\n",
      "3 h\n",
      "4 o\n",
      "5 n\n",
      "6  \n",
      "7 i\n",
      "8 s\n",
      "9  \n",
      "10 a\n",
      "11 n\n",
      "12  \n",
      "13 i\n",
      "14 n\n",
      "15 t\n",
      "16 e\n",
      "17 r\n",
      "18 p\n",
      "19 r\n",
      "20 e\n",
      "21 t\n",
      "22 e\n",
      "23 d\n",
      "24 ,\n",
      "25  \n",
      "26 h\n",
      "27 i\n",
      "28 g\n",
      "29 h\n",
      "30 -\n",
      "31 l\n",
      "32 e\n",
      "33 v\n",
      "34 e\n",
      "35 l\n",
      "36 ,\n",
      "37  \n",
      "38 g\n",
      "39 e\n",
      "40 n\n",
      "41 e\n",
      "42 r\n",
      "43 a\n",
      "44 l\n",
      "45 -\n",
      "46 p\n",
      "47 u\n",
      "48 r\n",
      "49 p\n",
      "50 o\n",
      "51 s\n",
      "52 e\n",
      "53  \n",
      "54 p\n",
      "55 r\n",
      "56 o\n",
      "57 g\n",
      "58 r\n",
      "59 a\n",
      "60 m\n",
      "61 m\n",
      "62 i\n",
      "63 n\n",
      "64 g\n",
      "65  \n",
      "66 l\n",
      "67 a\n",
      "68 n\n",
      "69 g\n",
      "70 u\n",
      "71 a\n",
      "72 g\n",
      "73 e\n",
      "74 .\n",
      "75  \n",
      "76 c\n",
      "77 r\n",
      "78 e\n",
      "79 a\n",
      "80 t\n",
      "81 e\n",
      "82 d\n",
      "83  \n",
      "84 b\n",
      "85 y\n",
      "86  \n",
      "87 g\n",
      "88 u\n",
      "89 i\n",
      "90 d\n",
      "91 o\n",
      "92  \n",
      "93 v\n",
      "94 a\n",
      "95 n\n",
      "96  \n",
      "97 r\n",
      "98 o\n",
      "99 s\n",
      "100 s\n",
      "101 u\n",
      "102 m\n",
      "103  \n",
      "104 a\n",
      "105 n\n",
      "106 d\n",
      "107  \n",
      "108 f\n",
      "109 i\n",
      "110 r\n",
      "111 s\n",
      "112 t\n",
      "113  \n",
      "114 r\n",
      "115 e\n",
      "116 l\n",
      "117 e\n",
      "118 a\n",
      "119 s\n",
      "120 e\n",
      "121 d\n",
      "122  \n",
      "123 i\n",
      "124 n\n",
      "125  \n",
      "126 1\n",
      "127 9\n",
      "128 9\n",
      "129 1\n",
      "130 ,\n",
      "131  \n",
      "132 p\n",
      "133 y\n",
      "134 t\n",
      "135 h\n",
      "136 o\n",
      "137 n\n",
      "138 '\n",
      "139 s\n",
      "140  \n",
      "141 d\n",
      "142 e\n",
      "143 s\n",
      "144 i\n",
      "145 g\n",
      "146 n\n",
      "147  \n",
      "148 p\n",
      "149 h\n",
      "150 i\n",
      "151 l\n",
      "152 o\n",
      "153 s\n",
      "154 o\n",
      "155 p\n",
      "156 h\n",
      "157 y\n",
      "158  \n",
      "159 e\n",
      "160 m\n",
      "161 p\n",
      "162 h\n",
      "163 a\n",
      "164 s\n",
      "165 i\n",
      "166 z\n",
      "167 e\n",
      "168 s\n",
      "169  \n",
      "170 c\n",
      "171 o\n",
      "172 d\n",
      "173 e\n",
      "174  \n",
      "175 r\n",
      "176 e\n",
      "177 a\n",
      "178 d\n",
      "179 a\n",
      "180 b\n",
      "181 i\n",
      "182 l\n",
      "183 i\n",
      "184 t\n",
      "185 y\n",
      "186  \n",
      "187 w\n",
      "188 i\n",
      "189 t\n",
      "190 h\n",
      "191  \n",
      "192 i\n",
      "193 t\n",
      "194 s\n",
      "195  \n",
      "196 n\n",
      "197 o\n",
      "198 t\n",
      "199 a\n",
      "200 b\n",
      "201 l\n",
      "202 e\n",
      "203  \n",
      "204 u\n",
      "205 s\n",
      "206 e\n",
      "207  \n",
      "208 o\n",
      "209 f\n",
      "210  \n",
      "211 s\n",
      "212 i\n",
      "213 g\n",
      "214 n\n",
      "215 i\n",
      "216 f\n",
      "217 i\n",
      "218 c\n",
      "219 a\n",
      "220 n\n",
      "221 t\n",
      "222  \n",
      "223 w\n",
      "224 h\n",
      "225 i\n",
      "226 t\n",
      "227 e\n",
      "228 s\n",
      "229 p\n",
      "230 a\n",
      "231 c\n",
      "232 e\n",
      "233 .\n",
      "234  \n",
      "235 i\n",
      "236 t\n",
      "237 s\n",
      "238  \n",
      "239 l\n",
      "240 a\n",
      "241 n\n",
      "242 g\n",
      "243 u\n",
      "244 a\n",
      "245 g\n",
      "246 e\n",
      "247  \n",
      "248 c\n",
      "249 o\n",
      "250 n\n",
      "251 s\n",
      "252 t\n",
      "253 r\n",
      "254 u\n",
      "255 c\n",
      "256 t\n",
      "257 s\n",
      "258  \n",
      "259 a\n",
      "260 n\n",
      "261 d\n",
      "262  \n",
      "263 o\n",
      "264 b\n",
      "265 j\n",
      "266 e\n",
      "267 c\n",
      "268 t\n",
      "269 -\n",
      "270 o\n",
      "271 r\n",
      "272 i\n",
      "273 e\n",
      "274 n\n",
      "275 t\n",
      "276 e\n",
      "277 d\n",
      "278  \n",
      "279 a\n",
      "280 p\n",
      "281 p\n",
      "282 r\n",
      "283 o\n",
      "284 a\n",
      "285 c\n",
      "286 h\n",
      "287  \n",
      "288 a\n",
      "289 i\n",
      "290 m\n",
      "291  \n",
      "292 t\n",
      "293 o\n",
      "294  \n",
      "295 h\n",
      "296 e\n",
      "297 l\n",
      "298 p\n",
      "299  \n",
      "300 p\n",
      "301 r\n",
      "302 o\n",
      "303 g\n",
      "304 r\n",
      "305 a\n",
      "306 m\n",
      "307 m\n",
      "308 e\n",
      "309 r\n",
      "310 s\n",
      "311  \n",
      "312 w\n",
      "313 r\n",
      "314 i\n",
      "315 t\n",
      "316 e\n",
      "317  \n",
      "318 c\n",
      "319 l\n",
      "320 e\n",
      "321 a\n",
      "322 r\n",
      "323 ,\n",
      "324  \n",
      "325 l\n",
      "326 o\n",
      "327 g\n",
      "328 i\n",
      "329 c\n",
      "330 a\n",
      "331 l\n",
      "332  \n",
      "333 c\n",
      "334 o\n",
      "335 d\n",
      "336 e\n",
      "337  \n",
      "338 f\n",
      "339 o\n",
      "340 r\n",
      "341  \n",
      "342 s\n",
      "343 m\n",
      "344 a\n",
      "345 l\n",
      "346 l\n",
      "347  \n",
      "348 a\n",
      "349 n\n",
      "350 d\n",
      "351  \n",
      "352 l\n",
      "353 a\n",
      "354 r\n",
      "355 g\n",
      "356 e\n",
      "357 -\n",
      "358 s\n",
      "359 c\n",
      "360 a\n",
      "361 l\n",
      "362 e\n",
      "363  \n",
      "364 p\n",
      "365 r\n",
      "366 o\n",
      "367 j\n",
      "368 e\n",
      "369 c\n",
      "370 t\n",
      "371 s\n",
      "372 .\n",
      "373 [\n",
      "374 2\n",
      "375 7\n",
      "376 ]\n",
      "377  \n",
      "378 p\n",
      "379 y\n",
      "380 t\n",
      "381 h\n",
      "382 o\n",
      "383 n\n",
      "384  \n",
      "385 i\n",
      "386 s\n",
      "387  \n",
      "388 d\n",
      "389 y\n",
      "390 n\n",
      "391 a\n",
      "392 m\n",
      "393 i\n",
      "394 c\n",
      "395 a\n",
      "396 l\n",
      "397 l\n",
      "398 y\n",
      "399  \n",
      "400 t\n",
      "401 y\n",
      "402 p\n",
      "403 e\n",
      "404 d\n",
      "405  \n",
      "406 a\n",
      "407 n\n",
      "408 d\n",
      "409  \n",
      "410 g\n",
      "411 a\n",
      "412 r\n",
      "413 b\n",
      "414 a\n",
      "415 g\n",
      "416 e\n",
      "417 -\n",
      "418 c\n",
      "419 o\n",
      "420 l\n",
      "421 l\n",
      "422 e\n",
      "423 c\n",
      "424 t\n",
      "425 e\n",
      "426 d\n",
      "427 .\n",
      "428  \n",
      "429 i\n",
      "430 t\n",
      "431  \n",
      "432 s\n",
      "433 u\n",
      "434 p\n",
      "435 p\n",
      "436 o\n",
      "437 r\n",
      "438 t\n",
      "439 s\n",
      "440  \n",
      "441 m\n",
      "442 u\n",
      "443 l\n",
      "444 t\n",
      "445 i\n",
      "446 p\n",
      "447 l\n",
      "448 e\n",
      "449  \n",
      "450 p\n",
      "451 r\n",
      "452 o\n",
      "453 g\n",
      "454 r\n",
      "455 a\n",
      "456 m\n",
      "457 m\n",
      "458 i\n",
      "459 n\n",
      "460 g\n",
      "461  \n",
      "462 p\n",
      "463 a\n",
      "464 r\n",
      "465 a\n",
      "466 d\n",
      "467 i\n",
      "468 g\n",
      "469 m\n",
      "470 s\n",
      "471 ,\n",
      "472  \n",
      "473 i\n",
      "474 n\n",
      "475 c\n",
      "476 l\n",
      "477 u\n",
      "478 d\n",
      "479 i\n",
      "480 n\n",
      "481 g\n",
      "482  \n",
      "483 p\n",
      "484 r\n",
      "485 o\n",
      "486 c\n",
      "487 e\n",
      "488 d\n",
      "489 u\n",
      "490 r\n",
      "491 a\n",
      "492 l\n",
      "493 ,\n",
      "494  \n",
      "495 o\n",
      "496 b\n",
      "497 j\n",
      "498 e\n",
      "499 c\n",
      "500 t\n",
      "501 -\n",
      "502 o\n",
      "503 r\n",
      "504 i\n",
      "505 e\n",
      "506 n\n",
      "507 t\n",
      "508 e\n",
      "509 d\n",
      "510 ,\n",
      "511  \n",
      "512 a\n",
      "513 n\n",
      "514 d\n",
      "515  \n",
      "516 f\n",
      "517 u\n",
      "518 n\n",
      "519 c\n",
      "520 t\n",
      "521 i\n",
      "522 o\n",
      "523 n\n",
      "524 a\n",
      "525 l\n",
      "526  \n",
      "527 p\n",
      "528 r\n",
      "529 o\n",
      "530 g\n",
      "531 r\n",
      "532 a\n",
      "533 m\n",
      "534 m\n",
      "535 i\n",
      "536 n\n",
      "537 g\n",
      "538 .\n",
      "539  \n",
      "540 p\n",
      "541 y\n",
      "542 t\n",
      "543 h\n",
      "544 o\n",
      "545 n\n",
      "546  \n",
      "547 i\n",
      "548 s\n",
      "549  \n",
      "550 o\n",
      "551 f\n",
      "552 t\n",
      "553 e\n",
      "554 n\n",
      "555  \n",
      "556 d\n",
      "557 e\n",
      "558 s\n",
      "559 c\n",
      "560 r\n",
      "561 i\n",
      "562 b\n",
      "563 e\n",
      "564 d\n",
      "565  \n",
      "566 a\n",
      "567 s\n",
      "568  \n",
      "569 a\n",
      "570  \n",
      "571 '\n",
      "572 b\n",
      "573 a\n",
      "574 t\n",
      "575 t\n",
      "576 e\n",
      "577 r\n",
      "578 i\n",
      "579 e\n",
      "580 s\n",
      "581  \n",
      "582 i\n",
      "583 n\n",
      "584 c\n",
      "585 l\n",
      "586 u\n",
      "587 d\n",
      "588 e\n",
      "589 d\n",
      "590 '\n",
      "591  \n",
      "592 l\n",
      "593 a\n",
      "594 n\n",
      "595 g\n",
      "596 u\n",
      "597 a\n",
      "598 g\n",
      "599 e\n",
      "600  \n",
      "601 d\n",
      "602 u\n",
      "603 e\n",
      "604  \n",
      "605 t\n",
      "606 o\n",
      "607  \n",
      "608 i\n",
      "609 t\n",
      "610 s\n",
      "611  \n",
      "612 c\n",
      "613 o\n",
      "614 m\n",
      "615 p\n",
      "616 r\n",
      "617 e\n",
      "618 h\n",
      "619 e\n",
      "620 n\n",
      "621 s\n",
      "622 i\n",
      "623 v\n",
      "624 e\n",
      "625  \n",
      "626 s\n",
      "627 t\n",
      "628 a\n",
      "629 n\n",
      "630 d\n",
      "631 a\n",
      "632 r\n",
      "633 d\n",
      "634  \n",
      "635 l\n",
      "636 i\n",
      "637 b\n",
      "638 r\n",
      "639 a\n",
      "640 r\n",
      "641 y\n",
      "642 .\n",
      "643 [\n",
      "644 2\n",
      "645 8\n",
      "646 ]\n"
     ]
    }
   ],
   "source": [
    "for index, token in enumerate(text):\n",
    "    print(index, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><iframe src=\"data:text/html;charset=utf-8;base64,PCFET0NUWVBFIGh0bWw+CjxoZWFkPiAgICAKICAgIDxtZXRhIGh0dHAtZXF1aXY9ImNvbnRlbnQtdHlwZSIgY29udGVudD0idGV4dC9odG1sOyBjaGFyc2V0PVVURi04IiAvPgogICAgCiAgICAgICAgPHNjcmlwdD4KICAgICAgICAgICAgTF9OT19UT1VDSCA9IGZhbHNlOwogICAgICAgICAgICBMX0RJU0FCTEVfM0QgPSBmYWxzZTsKICAgICAgICA8L3NjcmlwdD4KICAgIAogICAgPHNjcmlwdCBzcmM9Imh0dHBzOi8vY2RuLmpzZGVsaXZyLm5ldC9ucG0vbGVhZmxldEAxLjUuMS9kaXN0L2xlYWZsZXQuanMiPjwvc2NyaXB0PgogICAgPHNjcmlwdCBzcmM9Imh0dHBzOi8vY29kZS5qcXVlcnkuY29tL2pxdWVyeS0xLjEyLjQubWluLmpzIj48L3NjcmlwdD4KICAgIDxzY3JpcHQgc3JjPSJodHRwczovL21heGNkbi5ib290c3RyYXBjZG4uY29tL2Jvb3RzdHJhcC8zLjIuMC9qcy9ib290c3RyYXAubWluLmpzIj48L3NjcmlwdD4KICAgIDxzY3JpcHQgc3JjPSJodHRwczovL2NkbmpzLmNsb3VkZmxhcmUuY29tL2FqYXgvbGlicy9MZWFmbGV0LmF3ZXNvbWUtbWFya2Vycy8yLjAuMi9sZWFmbGV0LmF3ZXNvbWUtbWFya2Vycy5qcyI+PC9zY3JpcHQ+CiAgICA8bGluayByZWw9InN0eWxlc2hlZXQiIGhyZWY9Imh0dHBzOi8vY2RuLmpzZGVsaXZyLm5ldC9ucG0vbGVhZmxldEAxLjUuMS9kaXN0L2xlYWZsZXQuY3NzIi8+CiAgICA8bGluayByZWw9InN0eWxlc2hlZXQiIGhyZWY9Imh0dHBzOi8vbWF4Y2RuLmJvb3RzdHJhcGNkbi5jb20vYm9vdHN0cmFwLzMuMi4wL2Nzcy9ib290c3RyYXAubWluLmNzcyIvPgogICAgPGxpbmsgcmVsPSJzdHlsZXNoZWV0IiBocmVmPSJodHRwczovL21heGNkbi5ib290c3RyYXBjZG4uY29tL2Jvb3RzdHJhcC8zLjIuMC9jc3MvYm9vdHN0cmFwLXRoZW1lLm1pbi5jc3MiLz4KICAgIDxsaW5rIHJlbD0ic3R5bGVzaGVldCIgaHJlZj0iaHR0cHM6Ly9tYXhjZG4uYm9vdHN0cmFwY2RuLmNvbS9mb250LWF3ZXNvbWUvNC42LjMvY3NzL2ZvbnQtYXdlc29tZS5taW4uY3NzIi8+CiAgICA8bGluayByZWw9InN0eWxlc2hlZXQiIGhyZWY9Imh0dHBzOi8vY2RuanMuY2xvdWRmbGFyZS5jb20vYWpheC9saWJzL0xlYWZsZXQuYXdlc29tZS1tYXJrZXJzLzIuMC4yL2xlYWZsZXQuYXdlc29tZS1tYXJrZXJzLmNzcyIvPgogICAgPGxpbmsgcmVsPSJzdHlsZXNoZWV0IiBocmVmPSJodHRwczovL3Jhd2Nkbi5naXRoYWNrLmNvbS9weXRob24tdmlzdWFsaXphdGlvbi9mb2xpdW0vbWFzdGVyL2ZvbGl1bS90ZW1wbGF0ZXMvbGVhZmxldC5hd2Vzb21lLnJvdGF0ZS5jc3MiLz4KICAgIDxzdHlsZT5odG1sLCBib2R5IHt3aWR0aDogMTAwJTtoZWlnaHQ6IDEwMCU7bWFyZ2luOiAwO3BhZGRpbmc6IDA7fTwvc3R5bGU+CiAgICA8c3R5bGU+I21hcCB7cG9zaXRpb246YWJzb2x1dGU7dG9wOjA7Ym90dG9tOjA7cmlnaHQ6MDtsZWZ0OjA7fTwvc3R5bGU+CiAgICAKICAgICAgICAgICAgPG1ldGEgbmFtZT0idmlld3BvcnQiIGNvbnRlbnQ9IndpZHRoPWRldmljZS13aWR0aCwKICAgICAgICAgICAgICAgIGluaXRpYWwtc2NhbGU9MS4wLCBtYXhpbXVtLXNjYWxlPTEuMCwgdXNlci1zY2FsYWJsZT1ubyIgLz4KICAgICAgICAgICAgPHN0eWxlPgogICAgICAgICAgICAgICAgI21hcF9jYTAzMGRkODJlZjc0ODljYWRmZTEzN2M4MjM5Y2M1ZCB7CiAgICAgICAgICAgICAgICAgICAgcG9zaXRpb246IHJlbGF0aXZlOwogICAgICAgICAgICAgICAgICAgIHdpZHRoOiAxMDAuMCU7CiAgICAgICAgICAgICAgICAgICAgaGVpZ2h0OiAxMDAuMCU7CiAgICAgICAgICAgICAgICAgICAgbGVmdDogMC4wJTsKICAgICAgICAgICAgICAgICAgICB0b3A6IDAuMCU7CiAgICAgICAgICAgICAgICB9CiAgICAgICAgICAgIDwvc3R5bGU+CiAgICAgICAgCjwvaGVhZD4KPGJvZHk+ICAgIAogICAgCiAgICAgICAgICAgIDxkaXYgY2xhc3M9ImZvbGl1bS1tYXAiIGlkPSJtYXBfY2EwMzBkZDgyZWY3NDg5Y2FkZmUxMzdjODIzOWNjNWQiID48L2Rpdj4KICAgICAgICAKPC9ib2R5Pgo8c2NyaXB0PiAgICAKICAgIAogICAgICAgICAgICB2YXIgbWFwX2NhMDMwZGQ4MmVmNzQ4OWNhZGZlMTM3YzgyMzljYzVkID0gTC5tYXAoCiAgICAgICAgICAgICAgICAibWFwX2NhMDMwZGQ4MmVmNzQ4OWNhZGZlMTM3YzgyMzljYzVkIiwKICAgICAgICAgICAgICAgIHsKICAgICAgICAgICAgICAgICAgICBjZW50ZXI6IFszNi41LCAxMjcuMTIzXSwKICAgICAgICAgICAgICAgICAgICBjcnM6IEwuQ1JTLkVQU0czODU3LAogICAgICAgICAgICAgICAgICAgIHpvb206IDYuNDksCiAgICAgICAgICAgICAgICAgICAgem9vbUNvbnRyb2w6IHRydWUsCiAgICAgICAgICAgICAgICAgICAgcHJlZmVyQ2FudmFzOiBmYWxzZSwKICAgICAgICAgICAgICAgIH0KICAgICAgICAgICAgKTsKCiAgICAgICAgICAgIAoKICAgICAgICAKICAgIAogICAgICAgICAgICB2YXIgdGlsZV9sYXllcl9jMTkwNjJiYTI0YTE0NTI4YWY4MWNiYWQzZjM4NDFmMSA9IEwudGlsZUxheWVyKAogICAgICAgICAgICAgICAgImh0dHBzOi8ve3N9LnRpbGUub3BlbnN0cmVldG1hcC5vcmcve3p9L3t4fS97eX0ucG5nIiwKICAgICAgICAgICAgICAgIHsiYXR0cmlidXRpb24iOiAiRGF0YSBieSBcdTAwMjZjb3B5OyBcdTAwM2NhIGhyZWY9XCJodHRwOi8vb3BlbnN0cmVldG1hcC5vcmdcIlx1MDAzZU9wZW5TdHJlZXRNYXBcdTAwM2MvYVx1MDAzZSwgdW5kZXIgXHUwMDNjYSBocmVmPVwiaHR0cDovL3d3dy5vcGVuc3RyZWV0bWFwLm9yZy9jb3B5cmlnaHRcIlx1MDAzZU9EYkxcdTAwM2MvYVx1MDAzZS4iLCAiZGV0ZWN0UmV0aW5hIjogZmFsc2UsICJtYXhOYXRpdmVab29tIjogMTgsICJtYXhab29tIjogMTgsICJtaW5ab29tIjogMCwgIm5vV3JhcCI6IGZhbHNlLCAib3BhY2l0eSI6IDEsICJzdWJkb21haW5zIjogImFiYyIsICJ0bXMiOiBmYWxzZX0KICAgICAgICAgICAgKS5hZGRUbyhtYXBfY2EwMzBkZDgyZWY3NDg5Y2FkZmUxMzdjODIzOWNjNWQpOwogICAgICAgIAogICAgCiAgICAgICAgICAgIHZhciBtYXJrZXJfN2QzNTM1ZDE3Yjg4NDQ4M2E5MDI3NWZiOTBkMGFhMWYgPSBMLm1hcmtlcigKICAgICAgICAgICAgICAgIFszNi41LCAxMjcuMTIzXSwKICAgICAgICAgICAgICAgIHt9CiAgICAgICAgICAgICkuYWRkVG8obWFwX2NhMDMwZGQ4MmVmNzQ4OWNhZGZlMTM3YzgyMzljYzVkKTsKICAgICAgICAKICAgIAogICAgICAgIHZhciBwb3B1cF83Y2JiYjZhNTBjMjg0ZTZlOTI0YzRlZmJiNGM1MTI2NiA9IEwucG9wdXAoeyJtYXhXaWR0aCI6ICIxMDAlIn0pOwoKICAgICAgICAKICAgICAgICAgICAgdmFyIGh0bWxfZTU4YmE2ZjNiNzc0NGNjMWFiNDE1ZTk5OTNiMzBlNzIgPSAkKGA8ZGl2IGlkPSJodG1sX2U1OGJhNmYzYjc3NDRjYzFhYjQxNWU5OTkzYjMwZTcyIiBzdHlsZT0id2lkdGg6IDEwMC4wJTsgaGVpZ2h0OiAxMDAuMCU7Ij7smrDrpqzsp5E8L2Rpdj5gKVswXTsKICAgICAgICAgICAgcG9wdXBfN2NiYmI2YTUwYzI4NGU2ZTkyNGM0ZWZiYjRjNTEyNjYuc2V0Q29udGVudChodG1sX2U1OGJhNmYzYjc3NDRjYzFhYjQxNWU5OTkzYjMwZTcyKTsKICAgICAgICAKCiAgICAgICAgbWFya2VyXzdkMzUzNWQxN2I4ODQ0ODNhOTAyNzVmYjkwZDBhYTFmLmJpbmRQb3B1cChwb3B1cF83Y2JiYjZhNTBjMjg0ZTZlOTI0YzRlZmJiNGM1MTI2NikKICAgICAgICA7CgogICAgICAgIAogICAgCiAgICAKICAgICAgICAgICAgdmFyIG1hcmtlcl81MGNlN2MzZmE4Y2I0MGJjODU4MmY0MDc4OGExZjFiNSA9IEwubWFya2VyKAogICAgICAgICAgICAgICAgWzM2LjUsIDEyNy4xMjRdLAogICAgICAgICAgICAgICAge30KICAgICAgICAgICAgKS5hZGRUbyhtYXBfY2EwMzBkZDgyZWY3NDg5Y2FkZmUxMzdjODIzOWNjNWQpOwogICAgICAgIAogICAgCiAgICAgICAgdmFyIHBvcHVwXzQyZmMyMGY4MzNmNjQ0OGNhZDY4YjdjNDRlM2Y2ZDg4ID0gTC5wb3B1cCh7Im1heFdpZHRoIjogIjEwMCUifSk7CgogICAgICAgIAogICAgICAgICAgICB2YXIgaHRtbF9iODhmZjVlZDBiODk0NjM4ODUxYWRjYjgxMzQ5NmQ5MSA9ICQoYDxkaXYgaWQ9Imh0bWxfYjg4ZmY1ZWQwYjg5NDYzODg1MWFkY2I4MTM0OTZkOTEiIHN0eWxlPSJ3aWR0aDogMTAwLjAlOyBoZWlnaHQ6IDEwMC4wJTsiPuyasOumrOynkTwvZGl2PmApWzBdOwogICAgICAgICAgICBwb3B1cF80MmZjMjBmODMzZjY0NDhjYWQ2OGI3YzQ0ZTNmNmQ4OC5zZXRDb250ZW50KGh0bWxfYjg4ZmY1ZWQwYjg5NDYzODg1MWFkY2I4MTM0OTZkOTEpOwogICAgICAgIAoKICAgICAgICBtYXJrZXJfNTBjZTdjM2ZhOGNiNDBiYzg1ODJmNDA3ODhhMWYxYjUuYmluZFBvcHVwKHBvcHVwXzQyZmMyMGY4MzNmNjQ0OGNhZDY4YjdjNDRlM2Y2ZDg4KQogICAgICAgIDsKCiAgICAgICAgCiAgICAKICAgIAogICAgICAgICAgICB2YXIgbWFya2VyX2ZjNzRkMTRlNTBkNjQ3YmY5YzljYWEzMzEyMTVmZDAxID0gTC5tYXJrZXIoCiAgICAgICAgICAgICAgICBbMzYuNSwgMTI3LjEyNV0sCiAgICAgICAgICAgICAgICB7fQogICAgICAgICAgICApLmFkZFRvKG1hcF9jYTAzMGRkODJlZjc0ODljYWRmZTEzN2M4MjM5Y2M1ZCk7CiAgICAgICAgCiAgICAKICAgICAgICB2YXIgcG9wdXBfNWYwZTg5ZjlhNzg0NGEwOThkNjIxZTJkMGZkMTFkOTAgPSBMLnBvcHVwKHsibWF4V2lkdGgiOiAiMTAwJSJ9KTsKCiAgICAgICAgCiAgICAgICAgICAgIHZhciBodG1sXzA5NmYyMWYzNTQxODRmYTU5OTY3NmU2ZDdjMTM3OWEzID0gJChgPGRpdiBpZD0iaHRtbF8wOTZmMjFmMzU0MTg0ZmE1OTk2NzZlNmQ3YzEzNzlhMyIgc3R5bGU9IndpZHRoOiAxMDAuMCU7IGhlaWdodDogMTAwLjAlOyI+7Jqw66as7KeRPC9kaXY+YClbMF07CiAgICAgICAgICAgIHBvcHVwXzVmMGU4OWY5YTc4NDRhMDk4ZDYyMWUyZDBmZDExZDkwLnNldENvbnRlbnQoaHRtbF8wOTZmMjFmMzU0MTg0ZmE1OTk2NzZlNmQ3YzEzNzlhMyk7CiAgICAgICAgCgogICAgICAgIG1hcmtlcl9mYzc0ZDE0ZTUwZDY0N2JmOWM5Y2FhMzMxMjE1ZmQwMS5iaW5kUG9wdXAocG9wdXBfNWYwZTg5ZjlhNzg0NGEwOThkNjIxZTJkMGZkMTFkOTApCiAgICAgICAgOwoKICAgICAgICAKICAgIAo8L3NjcmlwdD4=\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x26465405390>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 폴리움\n",
    "import folium\n",
    "myMap = folium.Map(location=[36.5, 127.123], zoom_start = 6.49)\n",
    "folium.Marker([36.5, 127.123], popup='우리집').add_to(myMap)\n",
    "folium.Marker([36.5, 127.124], popup='우리집').add_to(myMap)\n",
    "folium.Marker([36.5, 127.125], popup='우리집').add_to(myMap)\n",
    "myMap\n",
    "# myMap = folium.Map(location=[36.5, 127.123], zoom_start = 11, tiles = 'Stamen Terrain') \n",
    "# myMap # 확대 축소 정도\n",
    "# 지도에 원출력 하는 등의 함수 \n",
    "# 다양한 모드 설정 가능 tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download() # 단어 인식을 위한 사전 가져오기 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자바설치경로 찾기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()\n",
    "# print(word_tokenize('How are you?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dogs vs cats data preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# C:\\Users\\user\\Desktop\\data\\dogs-vs-cats\\train\\train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
