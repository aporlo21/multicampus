# 핸즈온 머신러닝 

## 2장. 머신러닝 프로젝트 처음부터 끝까지

- 맵리듀스 

구글에서 대용량 데이터 처리를 분산 병렬 컴퓨팅에서 처리하기 위한 목적으로 제작하여 발표한 소프트웨어 프레임워크로 페타바이트 이상의 대용량 데이터를 신뢰도가 낮은 컴퓨터로 구성된 클러스터 환경에서 병렬처리를 지원하기 위해 개발되었다 [ 하둡 프로젝트 ]

리듀스를 사용하여 직접회귀분석을 구현할수 있지만 일반적으로 하둡에서는 스파크의 mllib을 사용하는 것이 편리하고 성능도 뛰어납니다.

- reduce : 중복해서 데이터 분산하는 법
- map : 분산처리된 데이터 합치는 작업 ... hadoob 내 java 를통해서 작업수행 ( include.spark )
- scalar 언어 로도 작업수행이 가능함 == spark



- 평균 제곱근 오차 (RMSE) 

오차가 커질수록 값이 더욱 커져 예측에 얼마나 많은 오류가 있는지 가늠케 해준다 

![RMSEì ëí ì´ë¯¸ì§ ê²ìê²°ê³¼](https://www.includehelp.com/ml-ai/Images/rmse-1.jpg)

N : RMSE를 측정한 데이터셋에 있는 샘플수 

Predict : 데이터셋에 있는 i번째 샘플의 전체 특성값의 벡터 = h(x)

Actual : 해당 레이블 ( 해당 샘플의 기대 출력값) = h(y)

RMSE(X,h) || X : 데이터셋에 있는 모든 샘플의 모든 특성값을 포함하는 행렬 

=> 가설함수 h를 사용하여 일련의 샘플을 평가하는 비용함수 ( cost ) 

샘플 하나가 하나의 행이라 i번째 행은 x의 전치와 같다 

h는 시스템예측함수이면서 가설함수(hypothesis) 라한다 

시스템이 하나의 샘플 특성 벡터 x를 받으면 그 샘플에 대한 예측값를 출력한다

- MAE(X, h)

이상치로 보이는 구역이 많다면 평균절대오차 (=평균 절대 편차)



=> 이 둘 모두 예측값의 벡터와 목표값의 벡터 사이의 거리르 재는방법이다. 

거리측정법 

1. 유클리디안 노름 ( RMSE ) 계산
2. 맨해트 노름

=> 노름의 지수가 클수록 큰 값의 원소에 치우치며 작은 값은 무시된다. 

그러므로 RMSE 가 MAE 보다 좀더 이상치에 민감하다 

하지만 종모양 분포의 양끝단처럼 이상치가 매우드물면 RMSE가 잘맞아 일반적으로 사용됨



CAUTION ) 데이터를 더 깊게 들여다 보기전에 테스트 세트를 따로 떼어 놓고 절대 들여다 보지 않는다 = 편향된 자세로 데이터를 들여다 볼수 있으므로 이를 방지하기위해 [=> 데이터스누핑] 편향

 

### 1. 

[1] 테스트 세트 만들기 

무작위로 샘플 선택하여 데이터셋의 20프로를 떼어놓는다

계층적 샘플링  : 비율을 고려한 샘플링 



모집단(population)은 분석의 대상, 즉 관심의 대상이 되는 전체그룹입니다. 

표본(sample)은 관심의 대상이 되는 그룹에 대한 분석을 위해 추출한, 한 set의 관찰치

모수(parameter)는 모집단을 설명하는 어떤 값인데 Classical한 관점에서는 fixed된 어떤 값통계치(statistic)는 모집단을 설명하는 어떤 값을 sample로부터 구한 값입니다. 

예를들어 모집단(population)의 평균은 모수(parameter)이며 

표본(sample)으로부터 구한 평균은 통계치(statistic)입니다. 

모평균(=모집단 평균) 뿐만아니라 모집단의 특성을 의미하는 어떤 값은 모수(parameter)입니다. 

참고로, Classical은 Bayesian에 대비되는 표현인데 Bayesian estimation에 대해서는 앞서 알아보았습니다.



[2] 데이터 이해를 위한 탐색과 시각화 

train data 에 대해서만 탐색과 훈련 시행 

1) 지리정보 - 산점도 - 클러스터링 알고리즘 (지역 / 인구밀도) ex) 캘리포니아 주택 가격

2) 상관관계 조사 - 데이터셋이 너무 크지 않으므로 모든 특성간 표준 상관계수 ( 피어슨의 r )

corr() 메서드 이용 :  -1 ~ 1 범위 

1에 가까우면 강한 양의 상관관계를 가진다

계수가 -1 에 가깝거나 1에 가까우면 강한 상관관계를 지닌것



[3] 상관관계가 강하다는 사실 증명

1. 위쪽으로 향하는 방향을 볼수 있고 포인트들이 너무 널리 퍼져 있지않다
2. 가격 제한 값이 수평선으로 잘보인다
3. 450불 근처 수평선이 보이고 350, 280불에도 있을수 있으므로 이상치로 가정하고 제거 할 필요성이 있다 



[4] 특성 조합으로 실험 

feature 사이 흥미로운 corelation 발견

ex) 특정구역 방개수는 얼마나 많은 가구수가 있는지 모른다면 그다지 유용하지않다

진짜 필요한건 가구당 방개수 (mean)

침대개수 자체도 유용하지 않다 즉 방개수와 비교하는게 낫다 or 가구당 인원도 흥미로운 특성조합 (feature combination)

프로토 타입을 만들고 실행한후 그결과를 분석해서 더 많은 통찰을 얻고 다시 이탐색 단계로 돌아온다



[5] 머신러닝 알고리즘을 위한 데이터 준비

함수 만들어 자동화

- 어떤 데이터 셋에 대해서도 데이터 변환을 손쉽게 반복
- 향후 프로젝트 사용가능 변환 라이브러리 점진적 구축
- 여러 가지 데이터 변환을 쉽게 시도해볼수 있고 어떤 조합이 가장 좋은지 확인하는데 편리



### 2.

[1] 데이터 정제 

대부분의 머신러닝 알고리즘은 누락된 특성 다루지 못해 이를 처리할수 있는 함수를 만든다

방법은? 

- 해당 구역 제거

- 전체 특성 삭제

- 어떤 값으로 채운다 ( fillna )

dropna()

drop()

fillna()

from sklearn.impute import SimpleImputer 

: 누락된 값을 손쉽게 다루도록 해준다 

먼저 누락된 값을 특성의 중간값으로 대체한다고 지정해줄수 있는 간편함수



imputer객체의 fit() 메서드를 통해 훈련데이터에 적용이가능하다 

```
imputer.fit(housing_num)
```



imputer는 결과를 객체의 statistics 속성에 저장한다 



### 3.

[1] 텍스트와 범주형 특성 다루기

범주형 특성은 텍스트라 중간값을 계산불가능하여 그냥 남겨둔 feature

머신러닝 알고리즘은 숫자형을 다루기에 이 카테고리를 텍스트 => 숫자로 바꾼다

이를 위해 각 카테고리를 다른 정수값으로 매핑해주는 판다스의 factorize() 메서드사용

factorize() 메서드 : 카테고리 리스트도 반환해준다 

머신러닝 알고리즘이 가까이 있는 두값이 떨어져 있는 두 값보다 비슷하다고 생각하는 알고리즘 

ex) 컴퓨터는 category 0, 1 을 가까운 의미로 받아들이지만, 카테고리 0,4 가 문자형 실제로는 더 가까운 의미를 지닌다

카테고리별 이진특성을 만든다 ( Boolean 형 ) 임계치 전후 특성 나눠

한 특성이 1 // 다른 한특성 0 => 원핫 인코딩

카테고리들을 '원 핫 벡터' 로 인코딩한다

fit_transform() 메서드는 2차원 배열을 넣어줘야한다 

1차원배열이므로 구조를 바꿔야한다 

출력을 보면 sparse (부족한) 



categoricalEncoder를 사용하여 한번에 처리할수 있다 ( 텍스트 -> 숫자 -> 원핫)

=> 희소 행렬(0/1) 출력하지만 (불필요한메모리 너무많아)

=> 밀집행렬을원할경우 (encoding='onehot-dense')

[2] 나만의 변환기 

특별한 정제 작업 or 특성조합 => 자신만의 변환기 생성할때가 있다 



특성을 추가하는것이 머신러닝 알고리즘에 도움이 될지 안지 해당 변환기 하이퍼파라미터로 쉽게 확인이 가능하다 



[3] 특성 스케일링

입력 숫자 특성들의 스케일이 많이 다르면 잘 작동하지 않는다

ex) 

방의개수 6 - 39320

중간 소득 0 - 15

=> 타깃 값에 대한 스케일링은 일반적으로 불필요하다

특성의 범위를 같도록 만들어주는 방법 : min - max 스케일링과 표준화 가 널리 사용됨

(be called by 정규화 normalization )

0 - 1 범위 사이에 들도록 값을 이동하고 스케일을 조정한다

데이터에서 최솟값을 뺀 후 최댓값과 최솟값의 차이로 나눈것 sklearn.MinMaxScaler 변환기

standardation : 먼저 평균을 뺀후 표준편차로 나누어 분포의 분산이 1 이 되도록한다

표준화는 이상치에 영향을 덜 받는다

min - max 스케일링은 0 - 15 사이 모든 값을 0 - 0.15로 만들지만, 

표준화는 크게 영향받지 않는다

사이킷런에는 표준화를 위한 standardScaler변환기가 있다 



[4] 변환 파이프라인

정확한 순서대로 실행되어야 한다 

pipeline class : 연속된 변환을 순서대로 처리할수 있도록 도와주는 클래스

dataframselector는 나머지는 버리고 필요한 특성을 선택하여 df를 numpy array로 바꾸는 식으로 데이터를 변환한다

수치형 특성을 선택한 dataframeselector로 파이프라인을 시작해서 다른 전처리 단계들을 나열한다

##############################################################

문제를 정의한후

데이터를읽어들이고 탐색했다

train set // test set 을 나누고 

변환 파이프라인을 통해, 머신러닝 알고리즘에 주입할 데이터를 자동으로 정제하고 준비



### 4

[1] 모델선택과 훈련 

| 모델역할   | 함수                                                        |
| ---------- | ----------------------------------------------------------- |
| 선형회귀   | LinearRegression ) mean_square_error_RMSE                   |
| 결정트리   | DecisionTreeRegressor ) 복잡한 비선형관계 찾을수있다        |
| 앙상블학습 | RandomForest ) 여러 다른 모델을 모아 하나의 모델을 만드는것 |



[2] 교차검증을 사용한 평가

결정 트리 모델을 평가하는 방법

train_test_split 함수

: 훈련세트를 더작은 훈련세트와 검증 세트로 나누고 더 작은 훈련셋에서 모델을 훈련시키고 검증 세트로 모델을 평가하는방법이있다

sklearn 교차검증 

K-fold cross validation 

: 훈련세트를 폴드라 불리는 10개 서브셋으로 무작위 분할한다 

결정 트리 모델을 10번 훈련하고 평가하는데 

매번 다른 폴드를 선택해 평가에 사용하고 나머지 9개 폴드를 훈련에 사용한다 

10개의 평가 점수가 담긴 배열이 결과가 된다

랜덤 포레스트를 깊이 들어가기 전에 여러 종류의 머신러닝 알고리즘으로 하이퍼파라미터 조정에 너무 많은 시간을 들이지 않으면서 다양한 모델 ( Support Vector Machine // Neural Network )시도  // 가능성 있는 2- 5개정도의 모델을 선정하는 것이 목적



### 5

[1] 모델 세부 튜닝

1) 그리드 탐색 ( GridSearchCV )

: 탐색하고자 하는 하이퍼 파라미터와 시도해볼 값을 지정하기만 하면된다

가능한 모든 하이퍼파라미터 조합에 대해 교차 검증을 사용해 평가하게 된다

=> 최적의 하이퍼 파라미터 조합을 탐색해준다

but 비교적 작은수의 조합을 탐구할떄 굿



2) 랜덤탐색 ( RandomizedSearchCV )

: 하이퍼 파라미터탐색공간이 커지면 이 라이브러리 사용해라

GridSearchCV 방식 - 가능한 모든 조합을 시도하는 대신 



Parameter

: 모델 내부 확인가능변수



데이터 분석을 통해 산출이 가능한값

모델 능력결졍

데이터로부터 학습되어지고

수작업이 아닌 모델의 일부로 저장



1. 인공신경망에서 가중치
2. SVM 에서의 Support Vector
3. 선형회귀 로지스틱 회귀분석에서의 결정계수 



Hyperparameter 

: 모델 외적 요소

데이터 분석을 통해 얻는 값이 아니고 사용자에의해 정해짐 

경험에 의한 정의

예측 알고리즘 모델링 문제점 해결 [grid search/ /random search]

1. 신경망학습에서 학습률
2. SVM 에서 코스트값
3. KNN 에서의 K의 개수 



3) 앙상블 법

최상의모델연결

개개의 모델이 각기 다른 형태의 오차를 만들때 앙상블 ㄱ



4) 최상의 모델과 오차분석

5) 론칭,모니터링,시스템유지보수 

시스템의 실시간 성능을 체크하고 성능이떨어졌을때 알람을 통지할수 있는 모니터링 코드 필요

새로운 데이터를 사용해 주기적으로 훈련시키지않으면 데이터가 오래됨에 따라 모델도 함께 낙후되는 것이 일반적

사람의 분석 파이프라인과 시스템 연결 

시스템의 입력데이터품질 평가 

가능하면 이 과정을 자동화시켜 모델 갱신 필요성을 줄이고 일관되며 무결적 데이터 인사이트 생성 가능 

해당 시스템 상태를 스냅샷으로 저장해서 ( 가상머신) 이전 상태로 쉽게 되돌림 



- 순서

데이터준비 단계

모니터링 도구 구축

사람의 평가 파이프라인 세팅

주기적인 모델 학습 자동화

* \이러한 전체 프로세스 올바른 구축이 더 낫고 중요하다 
* 





































































