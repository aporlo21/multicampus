# day 20

## session

세션( tf.Session )을 이용하여 

그래프 전체 또는 일부를 실행하여 계산 결과를 출력하는 방법을 다룹니다. 

tf.Session 클래스는 텐서플로우의 오퍼레이션, 즉 노드를 실행하기 위한 클래스입니다.



 tf.Session.run 메소드는 오퍼레이션 객체(tf.Operation)를 실행하거나  

텐서 객체(tf.Tensor)의 값을 구하기 위한 주요 메커니즘입니다. 

세션 사용이 끝나면 tf.Session.close() 호출하여 자원을 해제해줘야 합니다.



세션 객체는 자신만의 물리적  자원(GPU, 네트워크 연결 등)을 가지고 있기 때문에  

블럭이 끝나면 자동으로 자원을 해제해주는 with 블럭내에서 사용하는 것을 권장합니다. 

with 블럭내에서는 세션의  run 메소드 대신에 텐서의 eval 메소드를 사용할 수 있습니다. 



플레이스홀더( tf.placeholder )를  입력으로 사용하는 경우에는 

tf.Session.run 메소드에서 실행하려는 연산을 위해 필요한 플레이스 홀더의 이름과 해당 플레이스홀더에서 사용하게 되는 값들을 딕셔너리 형태로 지정해줘야 합니다. 



## gradient descent

- learbing rate 정하는게 중요한이유

1) step 을 매우 크게 할 경우 

경사를 내려가는 스텝이 너무 크면 기울기 최소값이 아니라 바로 다시 올라갈 수 있다 

더크면 파랑색 선을 따라 그래프를 벗어날수도있다 => overshotting



2) step을 매우 작게 할 경우

굉장한시간이 소요되고, 시간제한 존재시 경사면이 다 돌기도전에 멈출것

그래서 미리 cost함수를 실행하여 rate의 비율을 찍어보고 테스트후에 돌리는 것이 좋다 



정리) 해당기술 통해 데이터 전처리 // 어떠한 한점에서 시작하면, 최고로 낮은점에 도착하는것이 목표이지만 x1, x2값이 있을 때 값이 크게 차이가 나게된다면 그래프는 눌린 타원형으로 그려질것 

데이터의 분포도를 보고 정규화 하는것이 중요하다 

1) original data 

2) zero-centered data 

3) normalized data



## 정규화

```python
x_std[:, 0] = ( x[:, 0] - x[:, 0].mean() ) / x[:, 0].std()
```



## overfitting

머신러닝이 학습데이터에 너무 딱 맞는 모델을 만들다보면 실제 테스트 데이트내에서는 제대로 가동이 안되는경우 많다 

너무 정확한 -, +  사이 linear line 을 그리는데 집중하다보면 모델이 비선형이 될수 있으므로, 해당데이터에 대한 정확한 선형보다 전체적 그래프 상 선형모델을 창출하는 모델이 성공적인 모델이다



### overfitting 줄이는법

1) training data 많으면 좋다

2) features num 축소

3) 정규화 / 일반화(regularzation) 시킨다 => 비선형구조들을 선형 구조로 만들어내는것을 말함 



plot('ro') : R - red // O - 그래프의 마커모양 



## python - sklearn LabelEncoder, OnehotEncoder

python에서 범주형 변수를 인코딩 하기 위하여 더미변수를 만들거나, one hot encoding을 합니다. 

선형회귀 같은 기본적인 통계 모형에서는 더미변수를 많이 쓰지만, 일반적인 머신러닝/딥러닝 접근법에서는 one hot encoding을 많이함, sklearn 패키지를 통한 one hot encoding 인데요.

one hot encoding을 하기 전에 label encoding을 하는 이유는 one hot encoder의 인풋으로 숫자형만올 수 있기 때문입니다. 

 label encoder의 결과로 문자형 변수가 숫자형 변수 범주형으로 변경되게 되고

이를 one hot encoder에 fit_transform 해주면, 이와 같이 one hot encoding된 결과를 얻을 수 있습니다.  이 때, train_y 변수는 sparse matrix가되어 프린트하면 위와같이 나타납니다



|      | id     | tissue | class | class2 | x    | y    | r     |
| ---- | ------ | ------ | ----- | ------ | ---- | ---- | ----- |
| 0    | mdb001 | G      | CIRC  | B      | 535  | 425  | 197.0 |
| 1    | mdb002 | G      | CIRC  | B      | 522  | 280  | 69.0  |
| 2    | mdb003 | D      | NORM  | N      | NaN  | NaN  | NaN   |
| 3    | mdb004 | D      | NORM  | N      | NaN  | NaN  | NaN   |
| 4    | mdb005 | F      | CIRC  | B      | 477  | 133  | 30.0  |
| 5    | mdb005 | F      | CIRC  | B      | 500  | 168  | 26.0  |



**Code**

```python
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder()
onehot_encoder = preprocessing.OneHotEncoder()

train_y = label_encoder.fit_transform(info['class2'])
train_y = train_y.reshape(len(train_y), 1)
train_y = onehot_encoder.fit_transform(train_y)

```

**LabelEncoder 결과**

[0 0 2 2 0 0 2 2 2 2 0 2 0 0 2 0 2 0 2 0 2 0 2 1 2 0 2 2 1 2 0 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 0 2 2 2 0 2 2 2 2 2 0 2 2 1 2 2 1 2 2 2 2 0 0 2 0 2 2 2 2 2



**OnehotEncoder 결과 - 최종 train_y**

```python
  (0, 0)	1.0
  (1, 0)	1.0
  (2, 2)	1.0
  (3, 2)	1.0
  (4, 0)	1.0
  (5, 0)	1.0
  (6, 2)	1.0
  (7, 2)	1.0
  (8, 2)	1.0
  (9, 2)	1.0
  (10, 0)	1.0
  (11, 2)	1.0
  (12, 0)	1.0
  (13, 0)	1.0
```



### OnehotEncoder Reason

텍스트의 모든 단어를 중복을 허락하지 않고 모아놓는다면 이를 단어 집합이라 한다.

단어 집합은 서로 다른 단어들의 집합입니다. 기본적으로 book과 books와 같이 단어의 변형 형태도 다른 단어로 간주합니다. 

이 단어 집합에 고유한 숫자를 부여하는 일을 진행합니다. 

텍스트에 단어가 5,000개가 등장했다면, 단어 집합의 크기를 5,000이라고 합니다



단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식입니다. 이렇게 표현된 벡터를 원-핫 벡터(One-hot vector)



문자를 숫자(더 구체적으로는 벡터)로 바꾸는 원-핫 인코딩

원-핫 인코딩의 과정을 두 가지 과정으로 정리해보겠습니다.
(1) 각 단어에 고유한 인덱스를 부여합니다. (정수 인코딩)
(2) 표현하고 싶은 단어의 인덱스의 위치에 1을 부여하고, 다른 단어의 인덱스의 위치에는 0을 부여합니다.

코엔엘파이의 Okt 형태소 분석기를 통해서 우선 문장에 대해서 형태소 토큰화를 수행

각 토큰에 대해서 고유한 인덱스(index)를 부여하였습니다. 

토큰을 입력하면 해당 토큰에 대한 원-핫 벡터를 만들어내는 함수를 만들었습니다.



### 케라스(Keras)를 이용한 원-핫 인코딩(One-hot encoding)



 to_categorical() : **케라스(Keras)를 이용한 원-핫 인코딩(One-hot encoding)**

위와 같이 생성된 단어 집합(Vocabulary)에 있는 단어들로만 구성된 텍스트가 있다면, texts_to_sequences()를 통해서 이를 바로 인덱스의 나열로 변환가능합니다. 

여기서 주의할 점은 to_categorical()은 정수 인코딩으로 부여된 인덱스를 그대로 배열의 인덱스로 사용하기 때문에, 실제 단어 집합의 크기보다 +1의 크기를 인자로 주어야 한다는 점입니다

 t.fit_on_texts()는 인덱스를 1부터 부여합니다. 하지만 배열의 인덱스는 0부터 시작하므로 맨 마지막 인덱스를 가진 단어의 인덱스가 7인데, 이를 원-핫 벡터로 만들기 위해서는 총 8의 크기를 가진 배열이 필요합니다



### 원-핫 인코딩(One-hot encoding)의 한계

이러한 표현 방식은 단어의 개수가 늘어날 수록, 벡터를 저장하기 위해 필요한 공간이 계속 늘어난다는 단점이 있습니다. 

다른 말로는 벡터의 차원이 계속 늘어난다고도 표현합니다.

원 핫 벡터는 단어 집합의 크기가 곧 벡터의 차원 수가 됩니다.

자연언어 연구를 위해 특정한 목적을 가지고 언어의 표본을 추출한 집합이다. 컴퓨터의 발달로 말뭉치 분석이 용이해졌으며 분석의 정확성을 위해 해당 자연언어를 형태소 분석하는 경우가 많다.

가령, 단어가 1,000개인 코퍼스를 가지고 원 핫 벡터를 만들면, 모든 단어 각각은 모두 1,000개의 차원을 가진 벡터가 됩니다. 

**이런 방식은 단어의 유사성을 전혀 표현하지 못한다는 단점이 있습니다.**

늑대, 호랑이, 강아지, 고양이라는 4개의 단어에 대해서 원-핫 인코딩을 해서 각각, [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]이라는 원-핫 벡터를 부여받았다고 합시다.

이 때 이 원-핫 벡터 표현 방법을 통해서는 강아지와 늑대가 유사하고, 호랑이와 고양이가 유사하다는 것을 표현할 수가 없습니다

이러한 단점을 없앨 수 있는, 단어의 '의미'를 다차원 공간에 벡터화 하는 기법으로는 두 가지가 있습니다

첫째는 카운트 기반으로 단어의 의미를 벡터화하는 LSA, HAL 등이 있으며,

둘째는 예측 기반으로 단어의 의미를 벡터화하는 전통 NNLM, RNNLM, Word2Vec, FastText 등이 있습니다.

그리고 카운트 기반과 예측 기반 두 가지 방법을 모두 사용하는 방법으로 Glove라는 방법이 존재합니다.

