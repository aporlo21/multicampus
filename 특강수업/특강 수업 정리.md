# 특강 수업 정리

---

어차피 회사는 장기 근속을 원하지 않는다

의지 - 온라인 교육 가능케함 

toastmaster (international)  구글링

* 구절

sow a thought and u reap an act

sow an act and u reap a habit

sow a habit and u reap a character 

sow a character and u reap 

* 데이터 분석

문제를 먼저 파악해라 

종교사회학 - 미신 / 기도 / 우물 - 우물신 / 나무 - 나무신 => 토템

한의학 - 아파 - 진맥 - 명의 - 척보면 척 / 정답

양의원 - 진찰 - 청진기 - 약 - 돌팔이 

=>  왜 느린지 파악, // 무엇이 문제인지 파악 

SSD - 서버시. 디스크 조각나도 부팅시 컴퓨터 속도 느려지지않아 수명이 낮아짐 

LAN 

작업관리자를 통해. CPU  / RAM 영역 확인

어떤 프로그램이 CPU 사용하는지 학인 

CPU - CHECK - 바이러스 

기본을지켜라

각도를 틀어라 / 노력보다 방향을 파악하라 - > 씨앗사진

작은 변화가 큰성과로 이어짐 

작은각도의 변화

좋은 와인 숙성 - 각도 => 타인과 경쟁안하는 영역을 탐색하라 



라벤더밭 - 표본을 만들어라 - 전체 파악 - 전수조사가능

연산 할당 체제 - 빅데이터 

찾지 못하면 DATA가 아니다.

 ![](C:\Users\user\Desktop\hojin\WKID.png)

=>  Wisdom / 예측을 가능케해라

5G - IOT 의 양과 속도를 감당하기위한 기술 sensor 들이 data를 산출해낸다. 

**구글 photo (back up) 사용권유 - 빅데이터 인공지능** 

* Gartner 3V => data 5Vs

Volume (양)

Velocity (속도)

Variety (다양성)

Veracity (정확성)

Value (가치)

* 마우스 쓰지마라 
* window + Tab
* window + project
* 자주 쓰는 - 핀고정 / 
* window + 숫자키
* 잠잘시간확보
* 구글문서 - 공유 ( google.dock ) tool - voice typing
* ctrl + shift + T : 닫은창 회복
* ctrl + tab : 창 변환
* google keep : memo program
* google lens
* **구글 검색 잘하는 법**
* **1일 1TED**
* **구글 똑똑 부자** 검색
* 빅데이터 - 클리닝 - 데이터 처리  **TEXT EDITER / EXCEL**
* 융합의 시대 
* **Kaggle Titanic 생존자 예측** 

master - 아주 어린 남자아이 

* **생활코딩 정규식** 공부하기! - 모든 코딩에 쓰인다
* Excel // ms ' mrs ' 분별
* 데이터 분석 - 실제 예측 모델 **프로젝트 완성경험** - 내 데이터 분석 - 재미배가
* 상관관계 인식 - 예측 결과 발굴
* 공유 정신 - 답
* google meet - 화상 회의  지원
* 적당히 빨라라
* '세바시 하용호'
* 'obama ab test'
* 감성 _  조합 _ 전략 _ 공부 자발적 / 항상 검색
* **DATA Literacy** : 데이터 엮어  읽는, 내는 능력
* 신기한 개념 => 당연한 기초
* SNS - Social Media - 데이터 분석 경험 증빙 자료 // 계정하나 더파 
* 뉴스 < 소셜미디어 
* **공감능력** 
* 'R' program 활용
* 서울시 심야 버스 노선 / 시차 활용 사례 - 한국 s3 
* 'person of interest' 'watcha'  searching
* 예지 정비 // plane 항공
* 광범위한 흥미 open mind - idea bank 
* money ball 
* 윔블던 테니스 승부 예측 4초
* 'charting culture'
* datafication - '분석하고 싶으면 모아라' // 시작은 '나'의 데이터
* peter drucker 
* **확률과 통계** analog - > digital / **평균에서 표준편차 2배만큼 떨어져있다** ''
* 대한민국 - 
* 나 - 대표값 / 평균 => 산술 평균
* **median (중앙값)** 연봉협상시. / out liar (이상치)
* 표준편차 시그마 (6시그마) [**out liar** 책 찾아읽어]

![](C:\Users\user\Desktop\hojin\표준편차.png)

* out liar 특성분석 - 중요 이슈 ( 이변수에 대한 따로 분석)
* 중간의 선을 그을 수 있다 => 상관관계
* 독립변수간에 상관관계 있으면 안돼
* autism - 자폐증

* 원본/출처 확인
* 조사방법 (논문 - 일반적 샘플링의 오점)
* 누수정보 확인
* 상관관계 =/ 인과관계
* 상식에 근거한 생각  (분석의 가치 확인후 단계)
* 자신의 삶을 자주적으로
*  **TED 청취 / 30 DAY Callenge** - 한계와 틀에 갖힌 자신에서 탈피하기 위해 필요한 과제 
* small changes - sustainable / continous challenge in Next 30 Days.
* 
* 면접 / 다른부서 - 자기 잘보이려는 노력 / 허상 / 수업 / 

# Insight (나용찬 강사님)

내 분야에서 빅데이터 접목

3가지 CH 

* 다보스 포럼 ( 선진국 포럼) - '4차 산업혁명'' 책 백년주기에서 오십년으로 단축
* 알고들어가야 안깨지고 안아프다 '가드'
* 무인 자동차  신호등 / 정지하지않아 . 소모품의 장기지속 => 기술엔지니어 / 메카닉 => 사고 X 보험 X
* KUZWEIL : 위대한 발명가 - 구글 AS
* 특이점(Singularity) : 폰 노이만 / 컴퓨터 기본골격 생각 / **the singularity is near** 도서 => 토론 필요
* connectivity + interlligence 
* 독일 & 중국 & 일본 = 장기집권 국가의 4차 산업혁명 대응
* 독일 / industry 4,0 : 제조업에 ict 접목 (2011) 
* cps = 공간과 시간의 제약없이 연결가능한 정보 네트워크 iot 이용
* 4단계 계획 : 제조업을 서비화 시키자 - 플랫폼을 팔아먹는다 
* 지멘스 : 클라이언트와 직접 대면 x 반도체 만드는 장비 생산
* 데이터기반 / 소프트웨어를 독일산으로 하자 => 지멘스

중국

* 1단계 ) 제조 강국 대열 진입
* 2단계 ) 제조 강국 중간 수준 확립 / 트럼프 - 트위터 : 단방향성 ( 샤오미 - 기술 // 내수 => 수출 )
* 3단계 )  

일본 

* 잃어버린 20년 
* 로봇 제조 기술 + ict => 4차 산업혁명
* 7개 세부정책

빅데이터 / 인재육성 / 혁신&기술 개발가속화 / 금융기능강화 / 산업구조&취업구조 전환 원활화 / 

iot 사물인터넷 - 사물인터넷으로 빅 데이터를 얻고 이를 클라우드에 저장해 인공지능으로 분석 활용 

ioe ( 주체가 나에서 - 나 조차도 thing이 된다 )

health app - 개인정보 동의 - 데이터 수집 

ai - 1800년대 체스 기계 // 1950년대 인공지능 용어 등장

output 안나와 1970 software crisis  신경망 이론으로 인공지능 재발견 

2012 deep learinging 알고리즘 

deep mind (영국회사)  : 홈페이지 한장 짜리 회사  / 공동창업자 만 / 기술은 사람이 가진다

국내 & 해외 : m&a방식 차이 - 

* Alphabet - google ( )
* microsoft
* amazon go - seatle / amazon 본사 delaying 이유 - tracking failure issue / sensor fusion : plate 에 sensor 부착 offline 진출 고려중 wallmart 경쟁 - 전미 3위 유통업체 매매 

samsung : 임원 - 기업문화 적응을 위해 2,3 세 집중



2nd phase big data

- IEEE 논문 / ACM 논문
- 데이터 베이스를 전문적으로 연구하는 단체 : DB => L(Large)DB
- 인터넷 발달 => V(Very)LDB : 학회 = Good 논문
- Big Data : Gartner (기업사장님 컨설팅) - 4v veracity 진실성 ex) 미국 대선
- Mckinsey : data handling 엔지니어 관점에서 빅데이터 정의 분석
- IDC : 활용에 관한 관점 정의 분석 

- 4v variability 
- 빅데이터 - 프로세스 / 시간의 경과에 따라 모델도 변화가 필요 (mapping) 모델 순환시킴  

Reasoning / learning 

business 

기업의 목적 : origin ( to know the future )

미래를 맞추는 법 - 얇고 명료하게 자료 분석 ( 예측모델 ) - 이진분류 문제 로 소개 / think simply

* hurricane Frances

data analyze 공개 하지 않아, 

나의 경험을 떠나 객관성을 가져와라

수학공부 / 프로그래밍 언어

수학공식을 언어에 사용되는 hook point 가져오는게 key

400/300 국영수

* data analazye

내가 알고싶은 미래가 무엇인지 정확히 진단

Feature Vector Attribute / Target Value Class 설정 : 최우선과제

과거 수집된 데이터 / 우리집단에서 보유한 데이터Feature Vector 가 관련 미래Target Value 에 대한 상관관계가 있는가

전술 / ''영어''라는 '수학' '국어' 과목의 '합격여부'필드에 대한 영향력

비교 => 정렬 => 객관성

DDD1 : 데이터 안에서 찾아야 하는 결정사항

DDD2 : 대규모로 반복되어 분석에 기반한 의사결정의 정확도가 약간만 향상 되어도 큰 이득을 주는 결정사항

사올수 없는 데이터의 경우, 

* 비즈니스 문제를 각하위 작업으로 분할
* 자동화 가능(알고리즘/ 전처리 이후 프로그래밍 파트) 부분 과 창의력(경험) 필요부분 구분 ( 전처리(같은 알고리즘일 시라도 아웃풋 달라 ) - 알고리즘 )



* DATA Mining Algorithms 

분류와 계층확률 추정

**회귀분석**

​	분류(classification) : ex) 잔치 연다 안한다 

​	회귀분석: target value에 숫자(알고픈 미래) 가 오는게 회귀분석 ex) 고기 몇근 ? 얼마나 많이? 

​	유사도 매칭 silmilarity matching 

​	알려진 데이터에 기반해 비슷한 개인을 찾음 

=> 거리 기반( 수학적 )

​	군집화 clustering 

date는 있는데 target value 없을시 

동시발생그룹화 co occurrence Group / 연관성규칙 발견 장바구니 분석

프로파일링 profiling / 행위기술 비정상적인 시스템 침입감시 같은 비정상 행위  pattern 분석 : 침입탐지 / 인프라 서버관리 : CLOUD Server Management

연결예측

데이터 축소 -feature selection(수학) 열정보 / 항 축소 확률모델 사용해라 

인과 모델링 ( 그래프이론, 확률 ) - 주어진 event issue 에 대한 확률표가 있다 조합관계 - 바이오 / 결과자체가 그래프가 되어 시각화된 모델링이 가능 . 

supervised learning 

타겟벨류 결정이 제일중요 

머신러닝 패턴인식 데이터마이닝 표지제목은 다르지만 알고리즘은 같다 - 해석이 달라진다 어느각도에 따라 그게 달라짐 // 공부를 해야  여러권의 책을 알아야 한다 => 공부를 여러번해라 

**다중회귀분석법을 이용한 지역전력수요예측 알고리즘**



**3rd phase algorithm** 

예측 모델만들기

전략 : 빅데이터 알고리즘

supervised segmentation : data set 안에 target value 있는 것을 쓰겠따

데이터 조각화 segmentation : 가로세로 조각화

target value 

영향력 - 수청 데이터 => 수식 데이터 도

기법 - Entropy / Tree

model - 미래 예측이라는 목적 달성을 위해 실세계를 단순하게 표현

목적에 따라 관련없는정보는 생략후 관련 정보 유지 feature selection

설명 모델링 : 값을 추정이 아닌 현상이나 절차를 쉽게 설명

블랙박스 : 성능이 나오기는 하는데 과정이 나오지않아 ( 알고리즘 )

설명이 가능한것, 쉬운것 : 

**model induction** 모델유도 데이터로부터 모델을 만드는것(대다수) / 모델을 먼저 만들고 데이터를 피드백한다 



미래 예측시 영향력 측정 . 수청데이터 ( 수식 )을 빼내라 

entropy : 어떤 집합에 대한 무질서 정도를 추정 ++++ ----- 전자 조합 

1번이란 아이가 나올 확률 : p1

확률 : 전체중 1번아이가 나올 확률 

log나눗셈 연산 = 음수 => ( - * - = + )

+전자 나올 확률 = -전자  => 원자의 상태는 물리적으로 안정 

예측 분류 모델 : 플러스 마이너스 전자 - **target value** / 예측의 분류 실험을 많이 돌려야한다 

​	**1에 가까울수록 안정된 상태 ( 물리 )** ex) **50,000 이라는 기준선 - 세기위해서 기준선 부여** 

​	**0에 가까울수록 안정된 상태 ( IT )** => 책을 많이 봐라 

이름 / 나이 / 직장 / 통장 / = 상환여부 

김    / 29 / 대 20명중 15or 5 / 200 / 월 

박 / 37 / 즁 40명중 30or 10/ 1억 / 전

최 / 57 / 자 / 20억 / 자 

log2 => log10 // entropy 상환고객 

log 0.7 / log 2 = -.0.5145 => **0에가까울수록 분류 잘된거 / 예제** 

속성이 target value에 주는 영향력 

수식 하나더 필요! => 

정보 증가량 / 

info 구성핵심 - entropy : 0에 가까울수록 구별잘한거 

부모 entropy 자녀 엔트로피 c 를 다 더해서 p 엔트로피에서 빼라 = Info Gain(자녀엔트로피 합) - 가 켜지면 엔트로피 작아진다 => 구별잘한다 

자녀엔트로피 _c : 확률 / 로그식 : **counting**(숫자가 아니여도 ok) 이 되야 구할수 있다. // 초보분석가 실수 _ 확률모델에서는 countable toolbox 시켜야한다 전처리할수 있어야한다. 

수학공부시 - 수학수식하나를 말로 한페이지 채울수 있어야한다  

부모엔트로피 - 자녀엔트로피합 = 

13/30 = 0.43

17/30 = 0.57

행렬 [폐구간]

가중치 개념을 가져온다 

weight : 9/10 가중치 = 기울기 =>> 가변값 // 영향력 고려 

IG이 가장 작은것부터 지운다 Feature selection

data = > Decision Tree : 현재보고 있는 데이터 set 에서 : ROOT node - 꼭대기 / leaf or terminal node = target value 말단 [steady seller] [ try version / algorithm / 시각화 / imagination 표준화버젼 ]

각 data set 가 seperated 되면서 재정렬 // 분모가 바뀐다. 

target value가 동일하지 않으면 segmentation 필요 // 

분류 문제 - 데이터 조각시, yes & no 단일화가 이루어져야한다 = target value 예측 영향력 높일시 IG 가장 높은거 선택 // 

search algorithm / 1/2확률로 데이터 제거 / 

모델 표현법에 대한 구성 : 폰 통신장비 / 애플서버 디시전트리 쉬리 Rule Base image 

비슷한 data 수집대상 앞서기위해서  Toolbase // contents 자체에 집중 // R에

* 코드볼줄알아야한다 (눈)
* 수식한줄 => 정확한 알고리즘 이해, 
* 에이스의 설명 알아듣기!
* 