#7번
midwest$Hasian <- ifelse(mean(midwest$totperasian)<midwest$totperasian,"large","small")
midwest$Hasian
#8번
table(midwest$Hasian)
qplot(x=midwest$Hasian)
# 리스트 관련 변수 다루기
a<-list(1,2,3)
class(seq(5,10)) # 5-10 6개 수열이 들어감
b<-as.list(seq(5,10))
b
e<-as.list(seq(5,10))
length(e)
#데이터를 표현할시
length(e)-1
e[5]
e[[5]]
e[[length(e)-1]]<-99
e
#list에서 자료삭제 요구시
c<-as.list(seq(10,14))
c
#첫번째 요소에 10 데이터 삭제를 원할시
c[[1]]<-NULL
c
#만약에 13이라는 값을 삭제하고 싶다 (3)번인덱스
c[[3]]<-NULL
c
#subset을 이용해 특정범위의 자료를 가져올수 있음
d<-as.list(seq(20))
#indexing : 변수 뒤에 대괄혼
d[10:15]
#vector : 자료형이 동일한게 쭊 나열 되어있는것
#c함수를 통해 벡터 생성
a<-c(1,2,3)
a
matrix(1:6, ncol=3, byrow = TRUE) #6행 1열
c(1,2),c(3,4) #벡터
rbind(c(1,2),c(3,4))
cbind(c(1,2),c(3,4))
# length : 벡터 든 행렬 이든 상관없이 전체 원소 갯수가 구해진다
a<-c(1,2,3)
length(a)
#c(1,2),c(3,4) #벡터
b<rbind(c(1,2),c(3,4))
#c(1,2),c(3,4) #벡터
b<-rbind(c(1,2),c(3,4))
length(b)
#dim : 벡터의 경우에는 null, 행렬의 경우에는 크기 벡터 출력
dim(a)
dimm(b)
dim(b)
#벡터 인덱싱  : [[]] (범위를 잡아서 인덱스에 해당하는 자료출력) 서브셋팅 : []
#원소의 이름이 없다는 점빼고는 거의일치
a[1:2]
a[1]
a[[1]]
a<-rbind(1:4, 6:9)
a
a[[8]]
a
# 8을 참조하여 출력
a[]
a[1,]
a[,2]
a[2,2:4]
a[1:2,2:3]
a[2,3]
a[[2,3]]
a<-1:10
a
# 5라는 원소를 빼고싶다
a[5]
b<-a[-5] #특정요소를 뺀 나머지 출력 subsetting
b
a[7:9]
c<-a[-7:-9]
c
bl<-c(T,F,T,T)
k<-1:4
k[bl]
k[k%%2==0]
k<-k*10
k
rep(NA,10)
seq(0,100,length=5)
#임의의 난수 발생시 rnorm함수실행
set.seed(0)
source('C:/Users/user/hojin/Data Analyse - 박길식교수님/day 11.R', echo=TRUE)
#임의의 난수 발생시 rnorm함수실행
set.seed(0)
rnorm(10) #난수 10개 생성
rnorm(10) #난수 10개 생성
#임의의 난수 발생시 rnorm함수실행
set.seed(0)
rnorm(10) #난수 10개 생성
rnorm(10) #난수 10개 생성
rnorm(10) #난수 10개 생성
#임의의 난수 발생시 rnorm함수실행
set.seed(1)
rnorm(10) #난수 10개 생성
runif(10) #구간을 나누어서 균등하게 출력 범위가 (0~1)사이
runif(10) #구간을 나누어서 균등하게 출력 범위가 (0~1)사이
runif(10) #구간을 나누어서 균등하게 출력 범위가 (0~1)사이
runif(10) #구간을 나누어서 균등하게 출력 범위가 (0~1)사이
boxplot(runif(10))
#R vector 화 연산
#시간측정
x<-1:10000
y<-10001:20000
proc.time()
#성능 참조시 유용
startTime<-proc.time()
#성능 참조시 유용
startTime<-proc.time()
z<-rep(0,10000)
z[i]<-x[1]+y[1]
z[1]
z[i]<-x[i]+y[i]
#성능 참조시 유용
startTime<-proc.time()
z<-rep(0,10000)
#z[i]<-x[1]+y[1]
#z[1]
for(i in 1:10000){
z[i]<-x[i]+y[i]
}
proc.time()<-startTime
proc.time()-startTime
a<-c(1,2,3)
b<-c(4,2,1)
a==b
all(a==b)
exp(a) #자연상수 함수
log(a)
a<-c(0,1,2,3)
log(a) #
a<-c(0,1,2,3)
log(a) # -infinite : -무한대
log10(a)
x<-1:5
y<-rep(1,length(x))
x+y
y
x+2
x<-50:59
x
max(x)
which.mas(x)
which.max(x)
matrix(c(10,20,10,20),nrow=2)
sum(x[,1])
colSums(x)
x<-matrix(c(10,20,10,20),nrow=2)
sum(x[,1])
colSums(x)
#그룹단위로 무엇인가 연산할시 dplr패키지 // 그룹화
set.seed(123)
df<-data.frame(k1=c("x","x","y","y","x"),
k2=c("f","s","f","s","f"),
d1=rnorm(5),
d2=rnorm(5))
df
#그룹단위로 무엇인가 연산할시 dplyr패키지 // 그룹화 => filter head select groupby
library(dplyr)
#df
group_by(df, k1) #1인수 - data, 2인자 - 그룹화 기준 변수 3인자 -
#df
summarise(group_by(df, k1), myMean-mean(d1)) #1인수 - data, 2인자 - 그룹화 기준 변수 3인자 -
#df
summarise(group_by(df, k1), myMean=mean(d1)) #1인수 - data, 2인자 - 그룹화 기준 변수 3인자 -
df
summarise(group_by(df, k1, k2), myMean=mean(d1))
#그룹화 pibot table 활용
library(tidyr) # 정리정돈을 잘한 tyde #데이터 정리/정돈 패키지
group_by(df, k1, k2)
group_by(df, k1, k2), myMean=mean(d1)
summarise(group_by(df, k1, k2), myMean=mean(d1))
spread<-summarise(group_by(df, k1, k2), myMean=mean(d1)), k2, mean)
spread<-summarise(group_by(df, k1, k2), myMean=mean(d1)), k2, myMean)
spread(summarise(group_by(df, k1, k2), myMean=mean(d1)), k2, myMean))
spread(summarise(group_by(df, k1, k2), myMean=mean(d1)), k2, myMean)
spread(summarise(group_by(df, k1, k2), myMean=mean(d1)), k1, myMean)
#merge : 두 df의 공통 key를 사용하여 병합
df1<-data.frame(k=c('b','b','a','c','a','a','b'),
d1=0:6)
df1
df2<-data.frame(k=c('a','b','d'),d2=0:2)
df2
merge(df1,df2)
merge(df1,df2, all = T)
merge(df1,df2, all.x = T)
merge(df1,df2, all.y = T)
install.packages("tidytext")
library(tidytext)
get_sentiments("afinn") #사용할수 있도록 하는 패키지
#긍정이냐 부정을 점수로 표현함 -5 ~ +5
summary(get_sentiments("afinn"))
get_sentiments("afinn")
AFINN<-data.frame(get_sentiments("afinn"))
hist(AFINN)
hist(AFINN$score)
hist(AFINN$score, xlim = c(-6,6)) #histogram 포토출력
hist(AFINN$score, xlim = c(-6,6), col='blue') #histogram 포토출력
hist(AFINN$score, xlim = c(-6,6), col='blue', breaks=20) #histogram 포토출력
#긍정,부정 감정정수를 점수로 표현함 -5 ~ +5
get_sentiments("bing")
summary(get_sentiments("nrc")) #0은 정의 x
#긍정,부정 감정정수를 점수로 표현함 -5 ~ +5
get_sentiments("nrc")
library(tidytext)
get_sentiments("afinn") #사용할수 있도록 하는 패키지
#긍정,부정 감정정수를 점수로 표현함 -5 ~ +5
get_sentiments("nrc")
summary(get_sentiments("nrc")) #0은 정의 x
AFINN<-data.frame(get_sentiments("afinn"))
hist(AFINN$score, xlim = c(-6,6), col='blue', breaks=20) #histogram 포토출력
summary(get_sentiments("afinn")) #0은 정의 x
#긍정,부정 감정정수를 점수로 표현함 -5 ~ +5
get_sentiments("nrc")
oplex<-data.frame(get_sentiments("bing"))
table(oplex$sentiment)
emolex<-data.frame(get_sentiments("nrc"))
table(emolex$sentiment)
emolex$word<-[emolex$sentiment=="anger"]
emolex$word[emolex$sentiment=="anger"]
#연도별로 p / n 평가 분류
library(tm)
library(stringr)
library(dplyr)
my.text.location<-"papers"
mypaper<-VCorpus(DirSource(my.text.location))
inspect(mypaper)
my.text.location<-"papers"
mypaper<-VCorpus(DirSource(my.text.location))
library(dplyr)
my.text.location<-"papers"
mypaper<-VCorpus(DirSource(my.text.location))
my.text.location<-"reminds"
mypaper<-VCorpus(DirSource(my.text.location))
my.text.location<-"papers"
setwd("C:/remind/remind/papers")
my.text.location<-"papers"
mypaper<-VCorpus(DirSource(my.text.location))
inspect(mypaper)
my.text.location<-"papers"
mypaper<-VCorpus(DirSource(my.text.location))
library(dplyr)
my.text.location<-"papers"
mypaper<-VCorpus(DirSource(my.text.location))
#연도별로 p / n 평가 분류
library(tm)
library(stringr)
library(dplyr)
my.text.location<-"papers"
mypaper<-VCorpus(DirSource(my.text.location))
my.text.location<-"papers"
View(my.text.location)
setwd("C:/Users/user/hojin/Data Analyse - 박길식교수님")
#연도별로 p / n 평가 분류
library(tm)
library(stringr)
library(dplyr)
my.text.location<-"papers"
View(my.text.location)
mypaper<-VCorpus(DirSource(my.text.location))
inspect(mypaper)
mypaper[[1]]
str(mypaper[[1]])
mypaper[[1]]$content
class(mypaper[[1]]$content) # list
class(unlist(mypaper[[1]][1]))
mypaper[[1]][1]
length(as.character(mypaper[[1]][1]))
length(unlist(mypaper[[1]][1])) #1
mytxt<-c(NA,24)
mytxt
mytxt[2]
mytxt<-c(rep(NA,24))
mytxt
for(i in 1:24){
as.character(mypaper[[i]][1])
}
mytxt[3]
}
for(i in 1:24){
mytxt[i]<-as.character(mypaper[[i]][1])
}
mytxt[3]
mytxt[24]
install.packages("tidytext")
library(tidytext)
#data_frame:tidytext형태로 데이터를 구성
my.df.text<-data_frame(paper.id=1:24, doc=mytxt)
#deprecated - tidy 쓰지말고 tibble 써라
my.df.text
#deprecated - tidy 쓰지말고 tibble 써라
my.df.text %>% #doc에 포함된 문서를 읽어서 단어로 분리작업해야한다
unnest_tokens(word, doc) # word로 분리하세요
#deprecated - tidy 쓰지말고 tibble 써라
my.df.text.word<-my.df.text %>% #doc에 포함된 문서를 읽어서 단어로 분리작업해야한다
unnest_tokens(word, doc) # word로 분리하세요
#문서 단위의 텍스트를 단어로 구분함
my.de.text.text
#문서 단위의 텍스트를 단어로 구분함
my.df.text.word
get_sentiments("bing")
left_join(pd, pd_new, by="Name") #by 공통속성
#creating dataframe1
pd=data.frame(Name=c("Senthil","Senthil","Sam","Sam"),
Month=c("Jan","Feb","Jan","Feb"),
BS = c(141.2,139.3,135.2,160.1),
BP = c(90,78,80,81))
print(pd)
#creating dataframe2
pd_new=data.frame(Name=c("Senthil","Ramesh","Sam"),
Department=c("PSE","Data Analytics","PSE"))
print(pd_new)
left_join(pd, pd_new, by="Name") #by 공통속성
right_join(pd, pd_new, by="Name")
#문자형에 해당하는 month -> <NA> 발생
inner_join(pd, pd_new, by="Name")
my.df.text.word %>%
inner_join(get_sentiments("bing"))
inner_join(get_sentiments(("bing")) %>%
speread(sentiment,n)
inner_join(get_sentiments(("bing")) %>%
%
%
my.df.text.word %>%
count(word,paper.id, sentiments) %>%
myresult<-my.df.text.word %>%
count(word,paper.id, sentiments) %>%
speread(sentiment,n,fill-=0)
myresult<-my.df.text.word %>%
inner_join(get_sentiments(("bing")) %>%
count(word,paper.id, sentiments) %>%
speread(sentiment,n,fill-=0)
speread(sentiment,n,fill=0)
spread(sentiment,n,fill=0)
myresult<-my.df.text.word %>%
inner_join(get_sentiments("bing")) %>%
count(word,paper.id, sentiments) %>%
spread(sentiment,n,fill=0)
myresult.sa<-my.df.text.word %>%
inner_join(get_sentiments("bing")) %>%
count(word,paper.id, sentiments) %>%
spread(sentiment,n,fill=0)
myresult.sa<-my.df.text.word %>%
inner_join(get_sentiments("bing")) %>%
count(word,paper.id, sentiments) %>%
spread(sentiment,n,fill=0)
myagg
summarise(group_by(.sa, paper.id),
pos.sum=sum(positive)
pos.sent=sum(negative))
myfilenames<-list.files(path=my.text.location,
allfiles = TRUE)
myfilenames
pub.year
left_join(pd, pd_new, by="Name") #by 공통속성 #name 이 있는곳만 depart 생성
right_join(pd, pd_new, by="Name") #outer_join 과 비슷
#문자형에 해당하는 month -> <NA> 발생
inner_join(pd, pd_new, by="Name")
myagg<-summarise(group_by(myresult.sa,papaer.))
print(pd_new)
#베이지안
#RNN-S2S, W2V
#CNN
G3NN #흉내내기
left_join(pd, pd_new, by="Name") #by 공통속성 #name 이 있는곳만 depart 생성
#베이지안
#RNN-S2S, W2V
#CNN
#GANN #흉내내기
left_join(pd, pd_new, by="Name") #by 공통속성 #name 이 있는곳만 depart 생성
right_join(pd, pd_new, by="Name") #outer_join 과 비슷
#문자형에 해당하는 month -> <NA> 발생
inner_join(pd, pd_new, by="Name")
myagg<-summarise(group_by(myresult.sa,papaer.))
#문자형에 해당하는 month -> <NA> 발생
inner_join(pd, pd_new, by="Name")
myagg<-summarise(group_by(myresult.sa,papar.))
myagg<-summarise(group_by(myresult.sa,papar.))
myagg<-summarise(group_by(myresult.sa,papar.))
length(as.character(mypaper[[1]][1]))
length(unlist(mypaper[[1]][1])) #1
mytxt<-c(NA,24)
mytxt[2]
mytxt<-c(rep(NA,24))
mytxt
for(i in 1:24){
mytxt[i]<-as.character(mypaper[[i]][1]) #각각의 문서를 벡터화시킴
}
mytxt[24]
library(tidytext)
#data_frame:tidytext형태로 데이터를 구성
my.df.text<-data_frame(paper.id=1:24, doc=mytxt) #dplyr패키지내 내장함수
#deprecated - tidy 쓰지말고 tibble 써라
my.df.text.word<-my.df.text %>% #doc에 포함된 문서를 읽어서 단어로 분리작업해야한다
unnest_tokens(word, doc) # word로 분리하세요
#문서 단위의 텍스트를 단어로 구분함
my.df.text.word #doc에 있는 모든 내용이 3502*2
get_sentiments("bing")
my.df.text.word %>%
inner_join(get_sentiments("bing"))
#creating dataframe1
pd=data.frame(Name=c("Senthil","Senthil","Sam","Sam"),
Month=c("Jan","Feb","Jan","Feb"),
BS = c(141.2,139.3,135.2,160.1),
BP = c(90,78,80,81))
print(pd)
#creating dataframe2
pd_new=data.frame(Name=c("Senthil","Ramesh","Sam"),
Department=c("PSE","Data Analytics","PSE"))
print(pd_new)
#베이지안
#RNN-S2S, W2V
#CNN
#GANN #흉내내기
left_join(pd, pd_new, by="Name") # by 공통속성 #name 이 있는곳만 depart 생성
right_join(pd, pd_new, by="Name") # outer_join 과 비슷
#문자형에 해당하는 month -> <NA> 발생
inner_join(pd, pd_new, by="Name")
myagg<-summarise(group_by(myresult.sa,papar.))
my.text.location<-"papers"
mypaper<-VCorpus(DirSource(my.text.location))
inspect(mypaper)
mytxt<-c(NA,24)
mytxt[2]
mytxt<-c(rep(NA,24))
mytxt
for(i in 1:24){
mytxt[i]<-as.character(mypaper[[i]][1]) #각각의 문서를 벡터화시킴
}
mytxt[24]
mytxt
#data_frame:tidytext형태로 데이터를 구성
my.df.text<-data_frame(paper.id=1:24, doc=mytxt) #dplyr패키지내 내장함수
#data_frame == tibble <=> data.frame // text상태로 데이터를 저장
my.df.text
#deprecated - tidy 쓰지말고 tibble 써라
my.df.text.word<-my.df.text %>% #doc에 포함된 문서를 읽어서 단어로 분리작업해야한다
unnest_tokens(word, doc) # word로 분리하세요
#문서 단위의 텍스트를 단어로 구분함
my.df.text.word #doc에 있는 모든 내용이 3502*2
get_sentiments("bing")
my.df.text.word %>%
inner_join(get_sentiments("bing"))
my.df.text.word<-my.df.text.word %>%
inner_join(get_sentiments("bing"))
#deprecated - tidy 쓰지말고 tibble 써라
my.df.text.word<-my.df.text %>% #doc에 포함된 문서를 읽어서 단어로 분리작업해야한다
unnest_tokens(word, doc) # word로 분리하세요
myresult.sa %>% my.df.text.word %>%
inner_join(get_sentiments("bing")) %>%
count(word,paper.id,sentiment) %>%
spread(sentiment,n,fill=0)
myresult.sa <- my.df.text.word %>%
inner_join(get_sentiments("bing")) %>%
count(word,paper.id,sentiment) %>%
spread(sentiment,n,fill=0)
myresult.sa
myresult.sa <- my.df.text.word %>%
inner_join(get_sentiments("bing")) %>%
count(word,paper.id,sentiment)
myresult.sa
myresult.sa <- my.df.text.word %>%
inner_join(get_sentiments("bing")) %>%
count(word,paper.id,sentiment) %>%
spread(sentiment,n,fill=0)
myresult.sa
myresult.sa <- my.df.text.word %>%
inner_join(get_sentiments("bing")) %>%
count(word,paper.id,sentiment) %>%
spread(sentiment,n)
myresult.sa
myresult.sa <- my.df.text.word %>%
inner_join(get_sentiments("bing")) %>%
count(word,paper.id,sentiment) %>%
spread(sentiment,n,fill=0)
myresult.sa
myagg<-group_by(myresult.sa,paper.id)
myagg
myagg<-summarize(group_by(myresult.sa,paper.id),
pos.sum=sum(positive),
neg.sum=sum(negative),
pos.sent=pos.sum-neg.sum)
myagg
myagg
myfilenames<-list.files(path=my.text.location,
all.filers = TRUE)
myfilenames
myfilenames<-list.files(path=my.text.location,
all.filers = TRUE)
myfilenames<-list.files(path=my.text.location,
all.files = TRUE)
myfilenames
paper.name<-myfilenames[3:26]
pub.year<-str_extract_all(paper.name,"[[:digit:]]{4}")
pub.year
pub.year<-as.numeric(unlist(str_extract_all(paper.name,"[[:digit:]]{4}")))
pub.year
pub.year.df<-data.frame(paper.id, paper,name, pub.year)
paper.id<-1:24
pub.year.df<-data.frame(paper.id, paper,name, pub.year)
pub.year.df<-data.frame(paper.id, paper.name, pub.year)
pub.year.df
mydf<-inner_join(myagg,pub.year.df,by="paper.id")
mydf
library(ggplot2)
ggplot(data=mydf, aes(x=paper.id,y=pos.sent)) + geom_col()
